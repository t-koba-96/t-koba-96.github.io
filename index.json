
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    [{"authors":null,"categories":null,"content":"Reseacher for Hitachi, Ltd.\nInterested in developing services that contribute to reduce burden and improve operational efficiency, by understanding human actions and behavior.\nNow attracted to using action analysis in sports scenes, such as learning support and play review. Feel free to contact.\n","date":1574726400,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1574726400,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://t-koba-96.github.io/author/takuya-kobayashi/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/takuya-kobayashi/","section":"authors","summary":"Reseacher for Hitachi, Ltd.\nInterested in developing services that contribute to reduce burden and improve operational efficiency, by understanding human actions and behavior.\nNow attracted to using action analysis in sports scenes, such as learning support and play review.","tags":null,"title":"Takuya Kobayashi","type":"authors"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"72983a7d17a459ae1ae132f022b1d204","permalink":"https://t-koba-96.github.io/author/%E8%AB%96%E6%96%87/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/%E8%AB%96%E6%96%87/","section":"authors","summary":"","tags":null,"title":"論文","type":"authors"},{"authors":null,"categories":null,"content":"Graph Graphとは、ノード(点)とエッジ(線)で構成される構造。\nと表現できる。\n隣接行列 上記のような適当なグラフ構造について考える。エッジの方向について考えなければ、ノード間のエッジの接続については下位のような隣接行列で表現することができる。尚、対角線上の赤い領域も塗りつぶされているのは、自身への接続(自己ループ)を表している。\n畳み込みの前に、まずは隣接行列を使ってノードの特徴量をグラフ構造に合わせて集約してみる。その場合、単純に隣接行列ノード特徴量の内積を取ればよい(下図。)ただし、集約時に結果を足し合わせてしまうと、ノードの接続数が結果に反映されるため、正規化によって接続数による出力の偏りをなくす必要がある(図では簡略化のため接続数でわって平均を求めている。)\nグラフ畳み込み 上記では隣接行列(エッジ)の重みを全て均等に扱っていたが、各接続の重みづけを学習することによって、エッジの重みを畳みこみカーネルと見立ててグラフ畳み込みを行う。\n","date":1696982400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1570752000,"objectID":"7539a492813e18ab857aee9d7c6d9cc9","permalink":"https://t-koba-96.github.io/blog/graphconv/","publishdate":"2023-10-11T00:00:00Z","relpermalink":"/blog/graphconv/","section":"blog","summary":"Understand Graph Convolutional Network","tags":null,"title":"Graph Convolution について理解","type":"blog"},{"authors":["Nadine Behrmann","S. Alireza Golestaneh"],"categories":null,"content":"概要 Task アクションセグメンテーション\nInput 各フレームに対して、行動ラベルが一種類与えられている動画。\nOutput 一般的にはフレーム毎に行動ラベルの予測結果を返すのに対し、行動ラベルのシーケンス(Transcriptと呼ぶ)とタイムスタンプ(Durationと呼ぶ)を別々に得て、セグメントレベルでの行動予測結果を返すことで、OverSegmentation(予測結果のブレがおきる)の問題を解決する。\nData 以下の２パターンを検証\n教師あり(全てのフレームに行動ラベルあり) 弱教師あり(各Transcriptに属するフレームが１つずつのみ与えられている) 提案手法 ステップ (a) : 一般的なSegmentationModelではOverSegmentationの問題が発生しやすく、後段で処理が必要。\n(b) : そこでTransformeベースのseq2seqモデルを適用し、Decoderの出力としてTranscriptとDurationを同時に得ることを考える。ただし、以下の２点の要因から、このモデル構造単体では機能しない。\nNLPと違い、非常に長い類似したフレーム入力に対して、短い行動出力シーケンスが求められる点。 NPLと違い、データセットの規模が非常に小さく、扱える学習データが少ない点。 (c) : そこで、(a)のEncoderの出力をDecoderの入力として追加することで、デコーダでは異なる行動に対してより識別性の高い特徴を学習できる。これによりTranscriptでは改善を得るが、Durationについては以前精度が悪いままである。\n(d) : そこで、Decoderでの直接のDuration出力はあきらめる。DecoderではあくまでTranscriptを得ることを目的とし、DurationについてはEncoderの出力も同時に活用することを考える。\n(e) : Encoderで得たフレーム単位のクラス予測値とDecoderの出力との間で相互アテンションをとって、最終的なセグメントレベルでの行動予測結果を返す。\n損失関数 Encoder\n通常のACtionSegmentationと同様、CrossEntropyLossを採用。 Decoder\nこちらもセグメント単位でのCrossEntropyLossを用いる。 実験 Dataset Breakfast 50salads GTEA Result コメント 弱教師学習でseq2seqを用いた2Branch(Transcript+Duration)手法がでてきていたものが、transformerベースのseq2seqとなったというイメージ。 ActionSegmentationのタスクでTransformerを利用していく中で、この論文でも述べているようにデータの少なさ+非常に長い類似したフレーム入力をどう扱っていくかが焦点になりそう。\n","date":1659312000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1659312000,"objectID":"997d61a2ce811cb6315ae18100c2972c","permalink":"https://t-koba-96.github.io/publication/unified-fully/","publishdate":"2022-08-01T00:00:00Z","relpermalink":"/publication/unified-fully/","section":"publication","summary":"Few-shot Learning においてTransormerを用いた局所の表現学習方法を提案。","tags":null,"title":"Unified Fully and Timestamp Supervised Temporal Action Segmentation via Sequence to Sequence Translation","type":"publication"},{"authors":["Toby Perrett","Alessandro Masullo"],"categories":null,"content":"概要 CrossTransformerを応用したFew-ShotのActionRecognition手法の提案。\n各学習クラスのサンプルが少ない中で、Queryビデオがどのクラスであるかを認識する。 N-way、K-shotのFSLの時について考える。通常だとQueryビデオがNクラスのうちのどのクラスに属しているか、各クラスのSupportVideo一つずつと比較し、その最大値もしくは平均値を取って判断するが、Transformerのネットワーク構造を用いることですべてのSupportVideoをまとめて見たうえで判断することが可能になるよ、という話。\n手法 各動画は等間隔でフレームサンプリングされたものを使う。QueryVideoからクエリ、SupportVideoからキーとバリューを作成し、CrossTransormerと同様の構造を用いたAttentionの計算を行う。 この際、上図にはPair representationsとTriple representationsがあるが、Pair representationsでは各動画の中から2フレーム(ペア)のみを用いて特徴量化し、Triple representations では3フレームのみを用いて特徴量化する。そのため、各動画の特徴量をあらわすために扱うのは実際には2~3フレームである（Ablationで4の場合についても検証している）。\n実験 新規性 ","date":1627776000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1627776000,"objectID":"45eb732af3427c77f13be3f7ce9e4870","permalink":"https://t-koba-96.github.io/publication/temporal-relational-crosstransformers/","publishdate":"2021-08-01T00:00:00Z","relpermalink":"/publication/temporal-relational-crosstransformers/","section":"publication","summary":"CrossTransformerを応用したFew-Shotの行動認識手法の提案。","tags":null,"title":"Temporal-Relational CrossTransformers for Few-Shot Action Recognition","type":"publication"},{"authors":["Jingwei Ji"],"categories":null,"content":"概要 Visual Genomeの動画版のデータセット。またFeature Banksを用いた手法の提案。\n手法 データセット データセットのラベル内容としては動画中のActionの領域とそのCaption，またVisualGenomeのようなSceneGraphが与えられている。\n動画内の各Actionに関して，Actionを均等な時間間隔で分割した際の5フレームを抽出し，その5フレームにおけるシーングラフのラベルを作成。上記の例では１つの動画で４つのアクションが起きているため，4✖️5の合計20フレーム分のシーングラフが存在する。\n他のデータセットとの比較としては以下の通り。\n動画から各Actionを抽出するAction Recognitionのタスクを考える。同グループの以前の研究であるFeature Banksをベースラインとして用いている。従来のように抽出した3DCNN特徴に加えて，今回のラベルによって得られるシーングラフを予測させてobjectとrelationshipsを捉えたfeatured map を同様にFeature bank として長期の時系列情報として保持することで，行動の分類を行っている。\n実験 提案手法の評価。シーングラフ の利用によって精度の向上を確認。\n既存のシーングラフ生成モデルをActionGenomeに適用した際の精度。既存の手法は画像ベースの手法のため精度も低く，動画用に改善の余地ありとのこと。\n新規性 動画用のシーングラフを提案。シーングラフを用いたActionRecognitionの手法。\nコメント 公開されたら触ってみたい。\n","date":1596240000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1596240000,"objectID":"e28dd31504b7ff2f35ba465bcce33bd8","permalink":"https://t-koba-96.github.io/publication/action-genome/","publishdate":"2020-08-01T00:00:00Z","relpermalink":"/publication/action-genome/","section":"publication","summary":"Visual Genomeの動画版のデータセット。","tags":null,"title":"Action Genome: Actions as Composition of Spatio-temporal Scene Graphs","type":"publication"},{"authors":["Carl Doersch","Ankush Gupta"],"categories":null,"content":"概要 貢献 [1] SimCLRを用いた自己表現学習: クラスラベルを用いた学習を行うと、抽出した特徴量は一般化されたクラスの情報のみを保持したものとなり、その過程で局所的な特徴や表現をそぎ落としたものとなってしまう。そこでSimCLR同様の自己表現学習方法を導入する。自身の画像自体を正解レベルとして扱うことで、細部の表現をそぎ落とさないような学習を実現する。\n[2] Transformerによる局所マップ同士の表現の親和性を学習。例えば動物であれば、各体の部位の形や色等、その局所的な特徴の対応性から同一の動物であると決定することができる。Transformerを用いてこのような局所的な特徴同士の比較により、最終的にどのクラスに属しているかの決定を行う。\n手法 実験 新規性 ","date":1596240000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1596240000,"objectID":"d16eb539db95cd764a6cba234d38a2f4","permalink":"https://t-koba-96.github.io/publication/crosstransformers/","publishdate":"2020-08-01T00:00:00Z","relpermalink":"/publication/crosstransformers/","section":"publication","summary":"Few-shot Learning においてTransormerを用いた局所の表現学習方法を提案。","tags":null,"title":"CrossTransformers: spatially-aware few-shot transfer","type":"publication"},{"authors":["Takuya Kobayashi","Yoshimitsu Aoki","Shogo Shimizu","Katsuhiro Kusano","Seiji Okumura"],"categories":null,"content":"","date":1574726400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1574726400,"objectID":"1232d50e6a28296a60b8d9fdd30ee9eb","permalink":"https://t-koba-96.github.io/published/sitis2019/","publishdate":"2019-11-26T00:00:00Z","relpermalink":"/published/sitis2019/","section":"published","summary":"In SITIS2019","tags":[],"title":"Fine-grained Action Recognition in Assembly Work Scenes by Drawing Attention to the Hands","type":"published"},{"authors":null,"categories":null,"content":"概要 三菱電機との共同研究成果が公開されました。\n共同研究成果\n三菱電機プレスリリース\n日経ニューメディア\n","date":1571184000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1571184000,"objectID":"451708f76ea13f16c8de545d39294e82","permalink":"https://t-koba-96.github.io/blog/mitsubishi/","publishdate":"2019-10-16T00:00:00Z","relpermalink":"/blog/mitsubishi/","section":"blog","summary":"Joint research with Mitsubishi Electric","tags":null,"title":"三菱電機との共同研究成果がプレスリリースされました","type":"blog"},{"authors":null,"categories":null,"content":"動画認識デモについて 今まで作成したデモを汎用的に利用するために，最低限の機能のみに簡略化したデモを作成したので，使い方の手順とともに軽く説明していきたい。 まずはデモの実際の様子を以下に示した。フレーム単位での正解ラベルと予測ラベルを見ることができ，また手動でフレームを選ぶか自動再生するかの2つのモードを切り替えられる。\nlink 実行までの流れ 次に使い方についても軽く説明していきたい。\n➊\nまずはコードを公開しているGitHubページに移動し，gitレポジトリのクローンを行う。。\n$ git clone [リポジトリ名]\n➋\ngitレポジトリには実際に試すためのサンプルデータ（動画，正解ラベル，認識結果）は載せていないため，別にデータをダウンロードしてもらう必要がある。URL先のzipファイル(Assets内のdatas.zip)を解凍し，app.pyと同じ階層にdatasファイルが来るように配置する(元々あるdatasファイルと置き換える)。今回はサンプルデータとしてgteaデータセットを用いている。\n➌\n必要なコードとデータは揃ったので，最後にアプリを動かすために必要なパッケージのインストールを行う。\n$ pip install -r requirements.txt\n➍\n以上で準備は整ったので，アプリを実行する。\n$ python app.py\n➎\n“Running on [ローカルのURL] “，といったメッセージが表示されるので，指定されたURLを開けばアプリが実行されていることが確認できる。\n最後に Demo結果の様子だけ見たい場合はリンク先にあります。 フレームレートがまだまだ遅いので(Google App Engine の仮想マシン環境の影響もある)，より快適な可視化の為に改善をしていきたいとは思っている。\n","date":1566345600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1566345600,"objectID":"21facc1ff91e3275947459f7c2d3c92d","permalink":"https://t-koba-96.github.io/blog/video-demo/","publishdate":"2019-08-21T00:00:00Z","relpermalink":"/blog/video-demo/","section":"blog","summary":"Demo for action segmentation","tags":null,"title":"フレーム単位での行動認識結果のデモをFlaskを用いてデプロイ","type":"blog"},{"authors":["Jacob Devlin","Ming-Wei Chang"],"categories":null,"content":"概要 Bi-directional な Transformer を用いて，自然言語分野における新たな事前学習手法を提案。言語の事前学習によって，自然言語分野の複数のタスクにおいてSoTAを達成。\n手法 MLMとNSPという二つのタスクを解かせてトランスフォーマのエンコーダ側を事前学習。\nトランスフォーマで文章生成する際は，結局リカレントモデルと同じくそれ以前の単語の情報しか見れないという制約があるため，その制約を予め違う形でかけてPretrainしようというのがMLM。\n単一の文章における単語の関連性による出現確率についてだけでなく，２つの文章を比較させることによって文章全体としてより良い特徴表現を得ようということを目的としているのがNSP.\nMasked Language Model 入力の単語を予めいくつかマスキングしてしまい穴埋め問題を溶かせることによって，マスキングした部分の単語を前後の文脈の理解から復元させるタスク。入力シーケンスのうち，15%をランダムに選んでその部分の予測を行う。選んだ部分のうち80%はマスキング，10%は別の単語に置き換え，残りの10%はそのままの単語として入力させている。前後の文脈を考慮した特徴量抽出を行うことが目的。\nNext Sentence Prediction ２つの文章を並べて入力してあげて，２文目が１文目の次に来る文かどうかを予測するタスク。２つの文章を比較して予測結果を返すため，文章全体として良い特徴表現を得ることが期待できる。\n実験 新規性 新たな事前学習の枠組み。\n様々なタスクに転用するだけで精度向上。\n","date":1564617600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1564617600,"objectID":"761fc596a0711aa7f652b73712f6715d","permalink":"https://t-koba-96.github.io/publication/bert/","publishdate":"2019-08-01T00:00:00Z","relpermalink":"/publication/bert/","section":"publication","summary":"言語の事前学習によって，自然言語分野の複数のタスクにおいてSoTAを達成。","tags":null,"title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding","type":"publication"},{"authors":["Jacob Devlin","Ming-Wei Chang"],"categories":null,"content":"概要 Temporal action proposalのタスク。従来のような始点終点予測とは別に，Proposalの始点と長さを表すヒートマップを作成する。最終的に始点終点情報とヒートマップのConfidenceを組み合わせることで，Proposalを出してくる手法を提案。\n手法 ネットワーク全体の概要は以下の通り。\nBM confidence map ヒートマップは始点を横軸，Proposalの長さを縦軸として予測される。つまり同一の始点から始まる長さの違うProposalについても，ヒートマップを利用することによってConfidenceの算出が容易となる。このようにヒートマップはconfidenceのスコアを測るために用いる。\n実験 ActivityNetを用いて他手法との比較。\n出力例\n新規性 ヒートマップを用いたConfidenceの算出\n","date":1564617600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1564617600,"objectID":"b3faaf3e2e5fbc5c0158a1ef44198709","permalink":"https://t-koba-96.github.io/publication/bmn/","publishdate":"2019-08-01T00:00:00Z","relpermalink":"/publication/bmn/","section":"publication","summary":"Temporal action proposalのタスク。従来のような始点終点予測とは別に，Proposalの始点と長さを表すヒートマップを作成する。","tags":null,"title":"BMN: Boundary-Matching Network for Temporal Action Proposal Generation","type":"publication"},{"authors":["Daochang Liu"],"categories":null,"content":"概要 動画のクラスラベルのみのWeaklyなラベルを用いたTemporal Action Localization.\nWeaklyなラベルによる難点 Weaklyなラベルによる難点は主に２つあり，解決するための手法を提案。　⓵ Action completeness modeling 下図の上部参照。 例えばPKでは選手がシュートする動きとボールが飛んでいく動きによって構成されるが，Weaklyでは動画のクラス分類を元に解くのでより分類において重要であるシュートのところのみ切り取られやすい。そこでこれらの行動を分けて予測するようなマルチブランチのネットワーク構造にする。\n⓶ Action-context separation 下図の下部参照。行動の前景と背景ではフレーム全体としてのContextはあまり変わらない（どちらにもビリヤード台はずっと写ってる）。これに関しては人物や物の情報がアクションには関わっていると仮定し，OpticalFlowを用いた解決法を提案。\n手法 マルチブランチネットワークとアテンションモデルによって構成。マルチブランチNetworkではアクションをK分割して，それぞれの集合（Averageをとる）として求める全体のアクションを得るといった考え方。K分割したブランチにおいてそれぞれで違う部分に注目してもらう必要があるため，DiversityLossを採用。アテンションモデルでは時系列方向にSoftmaxをかけてフレームの重要度を算出。最後にこれらをかけあわせ，動画単位での MILLossをとる。\n推論時はマルチブランチをAverageした結果を利用。\n実験 新規性 マルチブランチによる推定。\n","date":1564617600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1564617600,"objectID":"9745f9053e1cd8b7d3b8f4dcf1bd54ac","permalink":"https://t-koba-96.github.io/publication/completeness-modeling/","publishdate":"2019-08-01T00:00:00Z","relpermalink":"/publication/completeness-modeling/","section":"publication","summary":"Predictive Codingで用いた手法を動画にも適用した論文。動画の次の潜在表現を回帰で予測させてあげて，相互情報量の最大化を目指す。","tags":null,"title":"Completeness Modeling and Context Separation for Weakly Supervised Temporal Action Localization","type":"publication"},{"authors":["Ji Zhang"],"categories":null,"content":"概要 シーングラフの生成において従来モデルの課題点を指摘した上で，それを改善するための新たなロスを提案し，SoTAを達成。\nシーングラフにおける言葉の定義 まずはじめに論文の内容とは関係ないが，Graph-RCNNではあいまいになっていて読みにくかったシーングラフにおける言葉の定義をしっかり記していてくれたのが地味にありがたい。\n・シーングラフを構成するにあたって必要にになるのは，objectとsubject,それをつなぐ関係性であるpredicateと，objectやsubjectに関するattributeの4つ。\n[object, subject, predicate, attribute]\n・実際に画像上に登場する物体(Object or Subject)は総称してentity.\n[enitity]\n・最終的に求めるべきobject,subject,predicateの関係性を\u0026lt;s,pred,o\u0026gt;と定義し，これのことをrelationshipsと呼ぶ。\n[relationships]\n従来手法の課題点 MotifNetのように，従来の手法ではまず初めにentityの情報を得てから，それをもとにpredicateをラベルのクラス分類問題として解くことでrelationshipsを求めるという２段階のアプローチが主流である。しかし，このようなアプローチには２つの課題が存在すると主張している。\n⓵ Entity Instance Confusion 同じラベルのentityが複数登場した時，relationshipにおけるenitityがそのうちのどれであるかの判断が難しくなる。以下の例では，男の人が持っているのではない方のワイングラスにobjectが付加されてしまっている。\n⓶ Proximal Rel Ambiguity 同様に同じラベルのpredicateを持ちうるobjectとsubjectのペアが複数ある時も，どのobjectとsubjectがペアであるかの判別が難しい。以下の例ではplaysというpredicateに対して3人の男と３つの楽器によるペアを判別する必要があるが，正しくマッチングできていないことがみて取れる。\n上記の２つともの問題点が発生する原因として，relationshipsを求める際にそれぞれ独立のクラス分類問題として解いてしまっていることがあげられる。そのため同じ物体や考えられるペアが複数存在してもそれぞれを独立の問題として解いてしまっており，お互いを比較した上でどちらがより正しいかを考慮できていない。\n本論文ではそれを解決するための新たなロスを提案している。\n提案するロス 新たなロス関数では３種類のロスを組み合わせたものとなっている。\n⓵ CLass Agnostic Loss クラスラベルを考慮しないトリプレットロス。背景（関連なし）クラスを除いた全てのpredicateクラスに関して，以下のような関数として定義する。\n今各entityにおいて，関係性を持つ他のentityとの間のスコアのうち最小のものを最大化し，関係性を持たない他のentityとの間のスコアのうち最大のものを最小化することを考える。そのためには，objectとsubjectに分けるとそれぞれ\nを最大化することと同義。そのため最小化するロス関数は\nとなる。\n⓶ Entity Class Aware Loss EntityInstanceConfusionを解決するためのロス。基本的には⓵と同じだが，対応するentityのクラスが同じ中でトリプレットロスを計算する。\n各クラスcに属するenitityに対して\nを計算するため，ロス関数は\nとなる。\n⓷ Predicate Class Aware Loss ⓶のpredicate版。ProximalRelAmbiguityを解決するためのロス。entityに付加されているpredicateのクラスが同じ中でトリプレットロスを計算する。\n各predicateクラスeが付加されているentityに対して\nを計算するため，ロス関数は\nとなる。\n最終的なロス 以上３つのロスを(L1,2,3)，従来のcrossentropyloss(L0)に加えることによって学習する。\nモデル RelDNと呼ばれるモデルを提案。\nSemantic, Spatial, Visualの３つのmoduleを用意し，最後に足し合わせたのちsoftmaxをかけてpredicateを出力。\nVisualのうち，relationships全体の特徴量の抽出のみ別に用意したCNN(構造は同じ)を用いて行う。理由としては関係性を捉えるための特徴としてはその限定された領域に注目して欲しく，全体をみてしまうCNNと区別したかったから。（以下図参照）\n実験 新規性 シーングラフ用の新たなロスの提案。\n","date":1564617600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1564617600,"objectID":"88fc7d4e9ac3170427aea345600420d1","permalink":"https://t-koba-96.github.io/publication/graphical-contrastive-loss/","publishdate":"2019-08-01T00:00:00Z","relpermalink":"/publication/graphical-contrastive-loss/","section":"publication","summary":"シーングラフの生成において従来モデルの課題点を指摘した上で，それを改善するための新たなロスを提案し，SoTAを達成。","tags":null,"title":"Graphical Contrastive Losses for Scene Graph Parsing","type":"publication"},{"authors":["Chao-Yuan Wu"],"categories":null,"content":"概要 ３DCNNでは4秒近くにおける時系列情報しか捉えられない。そこでよりLongTermな情報と組み合わせて考えることでVideoRecognitionの精度が上がりましたよという論文。\n手法 ３DCNNの元論文では，何フレームごとかに得た特徴をROI Poolingによってある区間における平均特徴量としたのち，全結合層で分類問題を解く（下図）。しかしこれではShortTermな情報しか捉えられず，ビデオ全体から判断が必要な情報が抜け落ちてしまう。\nそこでFeatureBankOperationを導入し，ShortTermとは別に得たLongTermな情報も同時に捉えるようなネットワークを用意してあげて，そこから得た特徴量も用いて認識を行う。\nFeatureBankOperatorの構造は以下。\n実験 新規性 ShortTerm+LongTerm\nコメント 単純な理論だが精度向上に結構寄与してる。\n","date":1564617600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1564617600,"objectID":"20dea59188d49ea0f3c7b6aca1ea8740","permalink":"https://t-koba-96.github.io/publication/long-term-feature-banks/","publishdate":"2019-08-01T00:00:00Z","relpermalink":"/publication/long-term-feature-banks/","section":"publication","summary":"３DCNNでは4秒近くにおける時系列情報しか捉えられない。そこでよりLongTermな情報と組み合わせて考えることでVideoRecognitionの精度が上がりましたよという論文。","tags":null,"title":"Long-Term Feature Banks for Detailed Video Understanding","type":"publication"},{"authors":["Yazan Abu Farha","Juergen Gall"],"categories":null,"content":"概要 行動認識のタスクにおいて,Temporal Convolutional Networksを複数重ねるMulti-TCNの提案。２個目以降のTCNには各クラスのPredictのSoftmax値を入力とすることで，OverSegmentation（予測結果の頻繁な変化）の抑止を実現。\n手法 下図のようにTCNを複数ステージ重ねていき，最終ステージの出力を予測結果として用いる。特徴として，２ステージ目以降の入力は前のステージのSoftmax出力を使う。\nイメージとしては，１ステージ目で大まかな行動の認識を行って，２ステージ目以降では時系列情報を考慮しながら細かい行動遷移の予測（主にOversegmentationの抑止）を行っている感じ？\nロス 交差エントロピーに加え，以下のようにフレーム間での正解クラスの予測値の変化を罰している。クラスが本当に切り替わるところは考慮しないよう，閾値を設けている。\n実験 データセット 50salads\nSingle-TCNとの比較 ロスの違いによる比較 新規性 ・クラス毎のsoftmax値を次のステージへの入力に用いている\n・遷移に制限をかけたロスの提案\n","date":1564617600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1564617600,"objectID":"81d276b08a8c94737c94ffbcb6d9aec9","permalink":"https://t-koba-96.github.io/publication/ms-tcn/","publishdate":"2019-08-01T00:00:00Z","relpermalink":"/publication/ms-tcn/","section":"publication","summary":"行動認識のタスクにおいて,Temporal Convolutional Networksを複数重ねるMulti-TCNの提案。","tags":null,"title":"MS-TCN: Multi-Stage Temporal Convolutional Network for Action Segmentation","type":"publication"},{"authors":["Debidatta Dwibedi"],"categories":null,"content":"概要 異なる動画においても，同じ動作の場合特徴空間上で近くなるように学習させることで，似た動作の動画同士を同期できるようなマッチングを実現。\n手法 まず動画のペアにおいて，各フレームの埋め込み表現をエンコーダによって得る。このうち１つのフレームに注目したとき（下図の右下赤い点），ペア動画においてその埋め込み表現から最も近いフレームを選ぶ。同様にして元の動画からもうひとつ点を選んだとき，最初の赤い点にもどることが理想であるが，違うフレームが選ばれたときはそこでロスをとることによって正解に近づけていくことを考える。\nしかし単純に上のように定義しただけでは損失関数の微分が不可能なため，勾配の計算ができない。そこでペア動画の各フレームにおいて距離空間に応じた重みを計算し，その重み付きの和を計算して，近似した近傍点を求める。同様に，近似した近傍点から元動画の各フレームの距離空間に応じた重みを計算し，その重み付きの和を計算することでロスをとる。\n実験 新規性 動画ペアにおいて，似た動作同士を近づける表現学習におけるCycle-Consistencyなロスの提案。\n","date":1564617600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1564617600,"objectID":"1e481daf11902464c91b1f498489349e","permalink":"https://t-koba-96.github.io/publication/temporal-cycle-consistency/","publishdate":"2019-08-01T00:00:00Z","relpermalink":"/publication/temporal-cycle-consistency/","section":"publication","summary":"異なる動画においても，同じ動作の場合特徴空間上で近くなるように学習させることで，似た動作の動画同士を同期できるようなマッチングを実現。","tags":null,"title":"Temporal Cycle-Consistency Learning","type":"publication"},{"authors":["Tengda Han"],"categories":null,"content":"概要 Predictive Codingで用いた手法を動画にも適用した論文。動画の次の潜在表現を回帰で予測させてあげて，相互情報量 の最大化を目指す。\n手法 全体としての考え方はPredictive Codingと一緒。ただし今回は動画を入力として扱うので，単純にフレームごとに系列データとして扱っていく。\nPositiveペアとNegativeペアによるMetric Learning。NegativeとしてはTemporalとSpatialがあり，フレーム内での同じ位置でも時間情報が違ったらNegativeとして遠ざけるよう学習する（背景に依存して動画全体として変化の少ない特徴表現となることを避けるため？）。\n実験 新規性 相互情報量最大化の考え方を動画にも応用。\n","date":1564617600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1564617600,"objectID":"35e0d20831df73c085fe187f96ae9d48","permalink":"https://t-koba-96.github.io/publication/video-representation-learning-by-dense-predictive-coding/","publishdate":"2019-08-01T00:00:00Z","relpermalink":"/publication/video-representation-learning-by-dense-predictive-coding/","section":"publication","summary":"Predictive Codingで用いた手法を動画にも適用した論文。動画の次の潜在表現を回帰で予測させてあげて，相互情報量の最大化を目指す。","tags":null,"title":"Video Representation Learning by Dense Predictive Coding","type":"publication"},{"authors":["Jacob Devlin","Ming-Wei Chang"],"categories":null,"content":"概要 Vision and Language において，多くのモデルでVisionとLanguageでネットワークをそれぞれ用意して同時に学習させていた部分を，Bertを応用することでマルチモーダルに事前学習させる方法を提案。基本的にはBertのMask穴埋め問題をLanguage+Visionに拡張したもの。\nDownstreamのタスクとしては入力テキスト情報に対応したビデオフレームの出力や，入力ビデオの次におきうるアクションのビデオ出力等が挙げられる。\n手法 ネットワーク全体の概要は以下の通り。\nマスク穴埋め問題 ランダムで入力をマスクして，出力を予測させることでSelf-supervisedに学習するのはBertと一緒。違いとして，文章同士で入力ではなく，文章と対応するビデオのセットでの入力となる。ビデオ側のトークンはクラスタリングで定義し，４階層✖️12次元の計20736次元の階層的Kmeansによって分類し，言語側と同じくクロスエントロピーをロスとして学習。\n実験 様々なDown Streamタスクでの比較。\nAction classification Video captioning 新規性 BertのようにSelf-supervisedな事前学習方法の提案によって大きなデータセット規模での学習を可能とし，精度の向上\nコメント Videoの入力があいまいなイメージ。Bertへの入力としてどのようなルールでフレームを選んでくるかは難しい問題に感じる。\n","date":1564617600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1564617600,"objectID":"43adfee511ce8bf93b3e9ad497e9c75a","permalink":"https://t-koba-96.github.io/publication/videobert/","publishdate":"2019-08-01T00:00:00Z","relpermalink":"/publication/videobert/","section":"publication","summary":"多くのモデルでVisionとLanguageでネットワークをそれぞれ用意して同時に学習させていた部分を，Bertを応用することでマルチモーダルに事前学習させる方法。","tags":null,"title":"VideoBERT: A Joint Model for Video and Language Representation Learning","type":"publication"},{"authors":["Takuya Kobayashi","Yoshimitsu Aoki","Shogo Shimizu","Katsuhiro Kusano","Seiji Okumura"],"categories":null,"content":"","date":1564358400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1564358400,"objectID":"4e08b8ec9faa2f679198197c317c97c0","permalink":"https://t-koba-96.github.io/published/miru2019/","publishdate":"2019-07-29T00:00:00Z","relpermalink":"/published/miru2019/","section":"published","summary":"In 画像の認識・理解シンポジウム2019","tags":[],"title":"詳細な作業遷移映像におけるアクションセグメンテーション","type":"published"},{"authors":["Takuya Kobayashi","Yoshimitsu Aoki","Shogo Shimizu","Katsuhiro Kusano","Seiji Okumura"],"categories":null,"content":"","date":1561939200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561939200,"objectID":"f5cfbd1aba3ee2088248b6612a8948b9","permalink":"https://t-koba-96.github.io/published/ssii2019/","publishdate":"2019-07-01T00:00:00Z","relpermalink":"/published/ssii2019/","section":"published","summary":"In 第25回画像センシングシンポジウム2019","tags":[],"title":"生産ラインの作業映像における詳細な行動の認識","type":"published"},{"authors":null,"categories":null,"content":"概要 Project for a simple handpose estimator. Openpose based hand pose estimator searches the hand position based on the skeleton keypoints which starts from the persons head. This means when some keypoints of the upper body is occluded, estimating the handpose will be difficult(Second example). To solve this problem, we combined openpose with TTFNet. By using the TTFNet to estimate the hand position directly, it will be able to estimate handpose in occluded cases too.\nFor the code, check GitHub Repository\n","date":1559692800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559692800,"objectID":"d72922b4a27ed9bc4be3d5cd738980a2","permalink":"https://t-koba-96.github.io/blog/handpose/","publishdate":"2019-06-05T00:00:00Z","relpermalink":"/blog/handpose/","section":"blog","summary":"Project for a simple handpose estimator","tags":null,"title":"OpenposeのHandPoseEstimatorを実利用のために改良してみた","type":"blog"},{"authors":["Takuya Kobayashi","Yoshimitsu Aoki","Shogo Shimizu","Katsuhiro Kusano","Seiji Okumura"],"categories":null,"content":"","date":1549497600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549497600,"objectID":"accf236e8d09ef15d7165c1adf9f0076","permalink":"https://t-koba-96.github.io/published/dia2019/","publishdate":"2019-02-07T00:00:00Z","relpermalink":"/published/dia2019/","section":"published","summary":"In 動的画像処理実利用化ワークショップ2019","tags":[],"title":"生産ラインの現場映像における姿勢情報と手元画像情報を用いた詳細行動認識","type":"published"},{"authors":["Jianwei Yang"],"categories":null,"content":"概要 GraphConvolutionを用いてSceneGraphの生成をより正確に行うための手法。SceneGraphとはVisualGenomeにおいて定義されているもので，{Subject,Relationship(Predicate),Object} (+Attribute) によるグラフ構造で表される。例として，A man is swinging a bat というような画像内の関係性があったとき， {man, swinging, bat} (+Attribute)というふうになる。これらの関係性を考えていき，最終的には(d)の関係性の出力を目指す。\n手法 全体の概要図配下の通り。３つのパートに分かれている。\n⓵ Object Region Proposal\n⓶ Relationship Proposal\n⓷ Graph Labeling\nObject Region Proposal Faster R-CNNを学習させて物体検出。出力としては物体の位置情報（BoundingBox），特徴量，クラスラベルの３つがある。\nRelationship Proposal 物体検出したもののうち，全てを繋ぐようなグラフ構造はパラメータ数的にも，現実世界における物体の関係性的にも現実的ではないため，本当に重要なエッジのみ抽出する段階。\n物体がObjectである場合とSubjectである場合（Relationship次第でどっちにもなり得るということ）は分けて考慮する必要があるため，構造は一緒だが異なるパラメータをもつ学習モデルを用意してあげて，それらのを通した行列積をエッジのスコアとして用いる。\n最終的に，スコアの高い上位k個のエッジを次のフェーズの入力として用いる。\nまた，物体ペアに関するNMSもここで行う。通常と違って，物体ペアによるIoU値を用いてNMSを行う。\nGraph Labeling AttentionalGCNを用いて最終的に物体，関係性の分類。GraphConvによってobject，relationshipがまわりのグラフ構造を考慮した上でそれぞれの特徴量を更新していく。\nA. Objectの更新について\n大きく３つの関係性を踏まえて特徴量の更新　⓵ 自分のまわりのObject(Message from other objects)\nただしObjectに関してはSkipconnectionを採用していて，離れたノード同士の関係性も考慮するようにしている。アテンションのαがskipとされているのもそのためである。特徴行列に掛け合わせる隣接行列のパラメータも学習させることで，隣接行列がノード同士の関係性を表すアテンションであるとみなすのがAttentionalGCN。そのため通常は同じノードを繋ぐ対角成分は１，エッジによってつながっていないノード同士は0となるようにし，残りの部分を学習させていくが，skipの場合は全ノード考慮するため，０成分で埋めずに対角成分以外をすべて更新していく。\n⓶ Subjectからみたrelationship\n⓷ Objectからみたrelationship\n注意としては⓶，⓷を区別する必要あり\nB. Relationshipの更新について\n⓵ Relationshipからみたsubject\n⓶ Relationshipからみたobject\n同様に⓶，⓷の区別の必要あり。\n損失関数・学習 参考文献より引用\n評価方法 VisualGenomeで用いられていたSGGenという評価方法を改善し，SGGen+と呼ばれる新たな評価軸を提案。\nSGGen 以下の二つの評価によって決定される。\n⓵　{Subject,Relationship(Predicate),Object}　のすべての要素が完全に一致している\n⓶　SubjectとObjectのローカライズ（位置予測）におけるIoUがともに0.5以上\nについて，両方満たしているときのみ正解として扱われ，１点加点される。しかしこれでは条件が厳しいため，見当違いな出力と１箇所だけ違う惜しい出力も同じとみなされ，結果の反映が正確にできていないと言える。\nSGGen+ そこで新しく，以下の３つの評価それぞれについて，満たしていれば随時１点加点される。\n⓵C(0)：正しく認識された(位置，クラス) Objectノードの数\n⓶C(P)：SubjectとObjectのローカライズ（位置予測）におけるIoUがともに0.5以上で，Relation(predicat)が正しいとき\n⓷C(T)：SGGenの評価\nこれらの合計点数をノードとエッジの合計Nで割ったもの.\n比較結果 SGGenとSGGen+の比較結果(SGGen+については分母)。SGGen+を用いた方が，より正解に近いグラフが出力できたときそれをより反映できるような評価値となっている。\n実験 新規性 GraphConvを用いたシーングラフの生成に関する初期の論文であり基本形。\nコメント 参考文献\nObjectという単語がSubjectと区別している場合としていない場合の２通りの使われ方がしていて紛らわしく感じた。\n","date":1533081600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1533081600,"objectID":"1738146322f197a0bd3ff5c7936b32d5","permalink":"https://t-koba-96.github.io/publication/graph-rcnn/","publishdate":"2018-08-01T00:00:00Z","relpermalink":"/publication/graph-rcnn/","section":"publication","summary":"GraphConvolutionを用いてSceneGraphの生成をより正確に行うための手法。","tags":null,"title":"Graph R-CNN for Scene Graph Generation","type":"publication"},{"authors":["Rowan Zellers"],"categories":null,"content":"概要 シーングラフ生成のタスクで既存のVisualGenomeデータセットにおける実験考察をもとに，新たにLSTMを用いた手法を提案。\nシーングラフの作成における考察 VisualGenomeを用いたシーングラフ生成の過程において，以下の２つの特徴があったと主張。\n⓵ Objectの情報からRelationを更新することはRelationを予測する上で有効であるが，逆はそうではない。 下図は，右下に書かれてる四角の中の右半分の情報(Head=始点のObject，Tail=終点のObject，Edge=Relation)を用いて残りの左半分を予測したときのtopK/精度を示したもの。\n見ると，headやtailからedgeを推測する際の精度が高いのに対し，edgeからhead, tailを予測するのは非常に難しくなっているのがわかる。\n既存手法ではobjectとrelationの相互関係性に注目し，relationからobject情報を更新していたが，この操作にはあまり意味がないことを主張している。\n⓶ 半分以上の画像において，一枚の中に複数回以上登場する同一の関係性（Motif）が存在している 動物と手足の関係や，木のように１つではなく大量に存在している物が写っている際，同じ関係性のMotifは何度も登場する可能性が高い。これらについて調査したところ，半分以上の画像においてこのような反復の関係性が確認された。\n手法 今までのGraphConvを使った手法(Graph-RCNNとか)と違いLSTMを用いた手法（下図）。\n構造としては，物体領域検出，物体クラス分類，関係性抽出の3段階構造。(B=BoundingBox, O=CLasslabel, R=Relation)\nObjectcontext予測のあとにEdgecontext予測を持ってきているのは，上記の考察における⓵の理由に基づいている。 以下分けてみていく。\n⓵ BoundingBox BoundingBoxの出力にはFaster-RCNNを利用。Faster-RCNNの出力はboundingboxの他に領域における特徴ベクトルと仮のクラスラベル（次のステップで更新していく）。\n⓶ Objects biLSTMを用いて⓵で得た仮のクラスラベルを更新していく。得たBoundingBoxで得た結果を系列情報として順番にbiLSTMに入力し，最終的なクラスラベルの予測を行う。各層の入力として⓵のクラスラベルの他に，前の入力におけるLSTMの最終出力（クラスのone-hot）を用いる。\n⓷ Relations 同様にbiLSTMを用いたedge情報の予測。入力として⓶で得たクラスのone-hotと，⓶のLSTMの隠れ層の出力を用いる。 最後に得た出力同士の全ての組み合わせを考えていき，関係性なしラベルを含んだエッジのラベルを予測させる。⓵で得たboundingbox，⓶で得たクラスラベル，⓷で得たEdgeの関係性ラベルを組み合わせて，最終的にシーングラフが作成できる。\n実験 VisualGenomeを用いて評価。\n[Scene Graph Detection] … ラベル情報なしでBBox, Object, Relation 全て予測\n[Scene Graph Classification] … BBoxラベルありでObject, Relation 予測\n[Predicate Classification] … BBox, Objectラベルありで Relation 予測\n生成例\n新規性 LSTMを用いた手法\n","date":1533081600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1533081600,"objectID":"ae904b5e3a99c1b81459f684f0111cda","permalink":"https://t-koba-96.github.io/publication/neural-motifs/","publishdate":"2018-08-01T00:00:00Z","relpermalink":"/publication/neural-motifs/","section":"publication","summary":"シーングラフ生成のタスクで既存のVisualGenomeデータセットにおける実験考察をもとに，新たにLSTMを用いた手法を提案。","tags":null,"title":"Neural Motifs: Scene Graph Parsing with Global Context","type":"publication"},{"authors":["Alexander Richard"],"categories":null,"content":"概要 クラスラベルの遷移順のみのWeaklyなラベルを用いたAction Segmentation. Viterbi algorism を用いた方法を提案。\n手法 LSTMか何かしらの時系列モデルを通して得た特徴量のsoftmax値に関して，クラスの遷移順を教師ラベルとして，その各クラスの長さを求めるようなvitervi algorism を定義。Viterbiを適用する前と適用したあとでCrossentropy Lossをとる。Viterbiを通す前の結果を通した後の遷移に近づけるイメージ。\n実験 新規性 Viterbiによる手法。\n今まではクラスの遷移ラベルから，フレーム単位での仮のラベルを使うことでロスを計算していたが，そうではなくviterbiを通す前と後でロスをとることで仮ラベルをなくし，精度向上。\n","date":1533081600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1533081600,"objectID":"4fbe239774d77c1a63bb0a1107bca628","permalink":"https://t-koba-96.github.io/publication/neuralnetwork-viterbi/","publishdate":"2018-08-01T00:00:00Z","relpermalink":"/publication/neuralnetwork-viterbi/","section":"publication","summary":"クラスラベルの遷移順のみのWeaklyなラベルを用いたAction Segmentation. Viterbi algorism を用いた方法を提案。","tags":null,"title":"NeuralNetwork-Viterbi: A Framework for Weakly Supervised Video Learning","type":"publication"},{"authors":["Aaron van den Oord"],"categories":null,"content":"概要 画像や音声における新しい表現学習の方法を提案。エンコーダーとGRUを組み合わせてGRUが次のエンコーダの出力を予測して,その相互情報量の最大化によって良い特徴表現を獲得する。\n手法 下図は音声入力の例。\nGRUの出力が次の潜在表現を回帰・予測し，その相互情報量の最大化を目指すことによって良い特徴表現を得る。\nロス NCEベースのロスを使用し，同じペアは近づけるように，ランダムにサンプルしたペアを遠ざけるよう学習。\n画像の場合 画像の場合そのままでは系列データにできないので，パッチごとにずらしていくことで系列データとして扱う。\n実験 画像 言語 t-SNEによる音声表現の可視化 新規性 系列データにおいて次の潜在表現を予測し，相互情報量の最大化を目的にすることによる良い表現の獲得。\n","date":1533081600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1533081600,"objectID":"e80901978f628e2d86066c3b380f47a6","permalink":"https://t-koba-96.github.io/publication/representation-learning-with-contrastive-predictive-coding/","publishdate":"2018-08-01T00:00:00Z","relpermalink":"/publication/representation-learning-with-contrastive-predictive-coding/","section":"publication","summary":"画像や音声における新しい表現学習の方法を提案。エンコーダーとGRUを組み合わせてGRUが次のエンコーダの出力を予測して,その相互情報量の最大化によって良い特徴表現を獲得する。","tags":null,"title":"Representation Learning with Contrastive Predictive Coding","type":"publication"},{"authors":["Sijie Yan","Yuanjun Xiong","Dahua Lin"],"categories":null,"content":"概要 グラフ畳み込みによる骨格ベースの行動認識手法を初めて提案した論文。\n時空間方向のグラフ畳み込みを利用したSkeleton-basedな行動認識手法を提案。\n手法 実験 新規性 コメント ","date":1533081600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1533081600,"objectID":"7cc590dc6692e20a634f36b01119e3e7","permalink":"https://t-koba-96.github.io/publication/spatial-temporal-graph-convolutional-networks/","publishdate":"2018-08-01T00:00:00Z","relpermalink":"/publication/spatial-temporal-graph-convolutional-networks/","section":"publication","summary":"時空間方向のグラフ畳み込みを利用したSkeleton-basedな行動認識手法を提案。","tags":null,"title":"Spatial Temporal Graph Convolutional Networks for Skeleton-Based Action Recognition","type":"publication"},{"authors":["Pierre Sermanet"],"categories":null,"content":"概要 Action recognition　におけるSelf-supervisedな事前学習方法の提案。異なる視点から撮った同じアクションにおける同フレームは同じアクションであるとみなせるため，同じフレーム同士は近づけて，ことなるフレームは遠ざけるといったトリプレットな学習方法ができる。\n手法 概要図は以下の通り。\n異なる視点のビデオを比べたとき，同じタイムスタンプである青フレームはPositive 同士，それに対して赤フレームはNegativeであるといえる。このような設定で学習させることで，同じ行動でもで視点が変わった際の対応関係を学習可能，より頑健なFeature抽出が可能に。\nロス トリプレットロスとして定義ができる。\nデータセット スマートフォンによるマルチビューな撮影\n実験 Action Alignment のタスクで評価。\n新規性 同時フレームに注目したSelf-supervisedな事前学習方法を提案。\n","date":1533081600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1533081600,"objectID":"afc7ebdb94be390ff520e210c2539727","permalink":"https://t-koba-96.github.io/publication/time-contrastive-networks/","publishdate":"2018-08-01T00:00:00Z","relpermalink":"/publication/time-contrastive-networks/","section":"publication","summary":"Action recognitionにおけるSelf-supervisedな事前学習方法の提案。","tags":null,"title":"Time-Contrastive Networks: Self-Supervised Learning from Video","type":"publication"},{"authors":["Li Ding","Chenliang xu"],"categories":null,"content":"概要 Action Segmentation のタスクにおいてWeakly-supervisedな手法. 学習用の動画デートセットの正解として，動画内の行動ラベルの順番のみ与える（各フレームにおける正解ラベルはなし）. 同様のWeakly-Supervisedな手法と比較して最高精度を記録。\n手法 ネットワーク全体の概要は以下の通り。\n本手法では，Action Segmentationを行う部分としてTCFPN ，認識結果を元にフレーム毎の正解ラベルの予測を行い，ground truth を更新する部分としてISBAをそれぞれ新たに提案している。\nTCFPN Action Segmentationを行う既存手法であるED-TCN と物体検出のタスクにおいて用いられるFeature Pyramid Networkを組み合わせた手法。単純にEncoder-Decoderのみを使うと，正確な特徴量を抽出できるものの，位置情報（今回の場合時間情報）が大雑把なものとなってしまう。そこで，Encoderの各層を1×1convして加えることで，より正確な位置情報を得ることが可能となる，というイメージ。\nTCFPNでSegmentationを行うには，フレーム毎の行動ラベルが必要なので，動画に対してN個の行動が順に起こるというWeaklyなラベルが与えられた時，動画のフレームをN等分して行動ラベルの初期値として与えてやる。その際0，１のみのOne-hotな表現ではなく，動画の行動が徐々に移り変わるだろうという予測を基に，以下のようなSoft Boundaryなラベル付けを行う。\nISBA TCFPNの出力を元にフレーム毎の正解ラベルの予測，更新を行う部分。要ははじめに与えたN等分するようなフレーム毎の行動の正解ラベルでは正確ではないため，TCFPNの出力を元にフレーム毎の正解を新たに予測し，更新することで実際のground truthに近いラベルを得ようという考え。\n学習・テスト TCFPNとISBAを繰り返し行い，認識結果を元にフレーム単位の行動ラベルを更新していくことで，フレーム単位の行動ラベルをground truthに近づけることと，Action Segmentationの精度の向上を同時に目指す。 ISBAにおいて独自のロスを導入し，3回連続でロスが小さくならなければ終了し，最もロスの小さかった時の結果を最終出力とする。\n実験 Breakfast datasetを用いて他のWeakly-Supervisedな手法との比較.\n学習時の評価 テスト時の評価 ちなみにfully-Supervisedな時の提案手法のaccuracyは52.0\n新規性 ・新たなWeakly-Supervisedな手法の提案。それにより最高精度を記録。\n・行動認識の結果を元に正解ラベルを更新していくという発想。\n","date":1533081600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1533081600,"objectID":"da76f4a10fd2cfb9651882bd3609d0c7","permalink":"https://t-koba-96.github.io/publication/weakly-iterative-soft-boundary-assignment/","publishdate":"2018-08-01T00:00:00Z","relpermalink":"/publication/weakly-iterative-soft-boundary-assignment/","section":"publication","summary":"Action Segmentation のタスクにおいてWeakly-supervisedな手法. 学習用の動画デートセットの正解として，動画内の行動ラベルの順番のみ与える（各フレームにおける正解ラベルはなし）.","tags":null,"title":"Weakly-Supervised Action Segmentation with Iterative Soft Boundary Assignment","type":"publication"},{"authors":["Ashish Vaswani","Noam Shazeer"],"categories":null,"content":"概要 機械翻訳用のネットワーク，トランスフォーマーの提案。従来LSTMやGRU等のリカレントネットワークや，畳み込みを主に用いていた自然言語の処理だが，アテンションのみを用いて単語間の関連性を考慮するような手法。\n手法 ネットワーク全体の概要は以下の通り。\nエンコーダ・デコーダ共に，この手法のキモとなるMulti-Head Attentionを残差接続したものを複数層重ねた構造になっている。 図中の Inputsは翻訳前言語の原文, Outputsは学習時においてGroundTruth・推論時はBeginning of sentence，Output Probabilitiesにおいて翻訳した文章を得る。\nアテンションの構造 アテンションの構造は以下のScaled Dot-Product Attention.　元の入力バッチからQ(Query),K(Key),V(Value)の３つを新たに用意して，そのうちQとKを用いて元のValueに掛け合わせるような重みマップ（Attention Map）のようなものを作成してあげる。\n単純にAttentionではなくMulti-Headと表現されているのは，このアテンション計算の際にh分割して計算したのち，最後にまたコンカットしていることを指している。（多分計算コストの削減？精度的な寄与も関係あるかも）\n既存手法に対する優位性 畳み込みとかリカレントネットワークに比べて長い時系列情報を見れるよねという話。あとリカレントネットワークは前の情報を引き継いで随時計算するのに対し，アテンションは一気に計算できるので，計算速度も全然早い。\n実験 新規性 アテンションのみによる新たなネットワークの提案。\nState of the art の更新。\n","date":1501545600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1501545600,"objectID":"cc6f0e216786c856b9b118362e98bc4a","permalink":"https://t-koba-96.github.io/publication/attention-is-all-you-need/","publishdate":"2017-08-01T00:00:00Z","relpermalink":"/publication/attention-is-all-you-need/","section":"publication","summary":"機械翻訳用のネットワークの提案。従来LSTMやGRUや畳み込みを主に用いていた自然言語の処理だが，アテンションのみを用いて単語間の関連性を考慮するような手法。","tags":null,"title":"Attention is all you need","type":"publication"},{"authors":["Colin Lea","Michael D. Flynn","Rene Vidal","Austin Reiter","Gregory D. Hage"],"categories":null,"content":"概要 詳細な行動認識のタスクにおいて、動画の各フレームから抽出した特徴を用いて、長期的な時系列情報を考慮するように畳み込みを行って認識を行うネットワークであるTemporal Convolutional Network(以後TCN)の提案.\n２種類のTCN , Encoder-Decoder TCN(以後ED-TCN) とDilated TCNを提案している.\n手法 提案した両方のTCNにおいて、入力は各フレームから何かしらのCNN(Resnet,VGG等)を用いて抽出した特徴量を時系列順に並べたものであり、出力としては行動クラス結果が各フレームごとに出力される.\nED-TCN エンコーダとデコーダを組み合わせたTCN. エンコーダで畳み込み＋MaxPoolingを行うことで時系列を考慮した特徴抽出、その後デコーダで畳み込み＋アップサンプリングを行うことで各フレームごとの行動クラス確率分布を得る.\nDilated-TCN 時系列の畳み込みとしてDilated Convolutionを用いた手法. Dilated Convolutionとは簡単に説明すると、入力の間隔を空けて(間を0とみなす)畳み込みを行うこと(d=1なら間隔なし、d=２なら間隔１、d=4なら間隔３). dの値をどんどん(本論文では指数関数的に)大きくしていったDilated Convolution層を重ねることで、より長期的な時系列の範囲で特徴抽出を行うことが可能となる. またスキップコネクションの導入で層を深くすることを可能に。\n実験 50salads、MERL Shopping、GTEAの３種のデータセットを用いて、Spatial CNN等との比較\n新規性 ・各層ごとに同時に処理を行うため、入力フレーム数が増えても処理時間に影響をあまり与えない.\n→フレームごとに処理を行うLSTM等と比較して高速な処理\n・３Dconvは動画全体(数フレーム分)に対して1つの行動クラスの出力であるのに対し、TCNでは各フレームごとに行動クラスの出力が可能\n","date":1501545600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1501545600,"objectID":"c2c7b8254c3429f0070da51f022c5d59","permalink":"https://t-koba-96.github.io/publication/temporal-convolutional-networks/","publishdate":"2017-08-01T00:00:00Z","relpermalink":"/publication/temporal-convolutional-networks/","section":"publication","summary":"詳細な行動認識のタスクにおいて、長期的な時系列情報を考慮するように畳み込みを行って認識を行うネットワークであるTemporal Convolutional Networkの提案.","tags":null,"title":"Temporal Convolutional Networks for Action Segmentation and Detection","type":"publication"},{"authors":["Ranjay Krishna"],"categories":null,"content":"概要 画像をもとにしたシーングラフやそれに関連したVQA等のラベルがついたVisual Genome データセットを作成。SceneGraphの研究分野で一般的に使われているデータセットとなっている。\n手法 データセット概要は下図。\nデータセットの内容については以下で構成されている。\nMultiple region graphs and their descriptions 様々な関係性が存在する一つの画像を，一つの説明文だけで完結させてしまうのは現実的ではない。そこで画像内にBoundingBoxを複数用意し，そのなかでのシーングラフ(Region Graphs)，説明文章(Region Descriptions)をそれぞれ作成している。ひとつの画像に対して平均で42個のこのような領域が用意されている。\nMultiple objects and their bounding boxes 画像内に平均で２１個の物体情報とそのBoundingBoxを用意。同一物体に対して複数ラベル存在する場合（man and person），大きい括りのラベル(person)が選択される。\nA set of attributes 画像内に平均で16個のAttributeラベルを用意。AttributeはObjectにつくものであり，一つのObjectに対して0個以上のAttributeが関連づいている。\nA set of relationships Relationshipとは２つのobjectをつなぐもの。有向グラフ構造のため，Relationshipをもっている２つのobjectの関係性の間には矢印が向く方と向けられる方が存在する。矢印の出発点がsubject，矢印の終着点がobjectと定義される。上の図の例で言えば，jumping over という関係性に関して，subject は man, object は fire hydrant となる。\nOne scene graph 今までRegionごとに得ていた情報をまとめて，画像全体として一つのScene Graph を作成。（上図の一番下）\nデータセット 一部の例。より詳しい内容に関しては元論文をチェック。\n新規性 シーングラフ用の新たなデータセットの提案。\n","date":1470009600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1470009600,"objectID":"0d1aec118529ccb285aeddbb001f3470","permalink":"https://t-koba-96.github.io/publication/visual-genome/","publishdate":"2016-08-01T00:00:00Z","relpermalink":"/publication/visual-genome/","section":"publication","summary":"画像をもとにしたシーングラフやそれに関連したVQA等のラベルがついたVisual Genome データセットを作成。","tags":null,"title":"Visual Genome : Connecting Language and Vision Using Crowdsourced Dense Image Annotations","type":"publication"},{"authors":["Karen Simonyan","Andrew Zisserman"],"categories":null,"content":"概要 Spatial stream ConvNet(以後Spatial Conv)とTemporal stream ConvNet(以後Temporal Conv)の組み合わせ. Spatial Convでは各フレームの静止画（RGB画像）を入力して畳み込み、空間情報の抽出によるクラス分類. Temporal Convでは各フレームのオプティカルフローを入力して畳み込み、動き情報の抽出によるクラス分類. 下図のように，Spatial Convにおける1つの入力フレームに対して，Temporal ConvではそのフレームからNフレーム分のオプティカルフローを用いる。（RGBとオプティカルフローの入力が1:N）。 最終的な結果は、それぞれのネットワークのクラスの確率分布を統合し、最も高確率のクラスを出力.\nTemporal Convにおいて、オプティカルフローは各フレームにおいてそれぞれ X,Y 方向に2次元配列として入力. よって、入力動画のRGBフレーム数がTの時、入力するオプティカルフローのフレーム数は2NT\n手法 実験 UCF-101、HMDB-51データセットを用いてハンドクラフト特徴量を用いた行動認識（IDT等）と比較.\nUCF-101データセット HMDB-501 またSpatial Conv、Temporal Convそれぞれ単独で用いた場合とも比較.\nUCF-101においては最も高精度. HMDB-51においてはハンドクラフトの方が高精度の場合も\n新規性 ・当時はハンドクラフト特徴量による認識が主流の中、深層学習を用いた手法.\n・オプティカルフローを用いることで動画の時系列情報を捉えようとするアプローチ\n","date":1404172800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1404172800,"objectID":"59759fe8241bafe76314833e85089e27","permalink":"https://t-koba-96.github.io/publication/two-stream-convolutional-networks/","publishdate":"2014-07-01T00:00:00Z","relpermalink":"/publication/two-stream-convolutional-networks/","section":"publication","summary":"空間情報（静止画）と時間情報（ フレーム間の動き）をそれぞれ畳み込んだ結果を統合することによる行動認識の手法を提案.","tags":null,"title":"Two-Stream Convolutional Networks for Action Recognition in Videos","type":"publication"}]
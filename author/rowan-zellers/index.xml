<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Rowan Zellers | Bianca Blog</title>
    <link>https://t-koba-96.github.io/author/rowan-zellers/</link>
      <atom:link href="https://t-koba-96.github.io/author/rowan-zellers/index.xml" rel="self" type="application/rss+xml" />
    <description>Rowan Zellers</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Wed, 01 Aug 2018 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://t-koba-96.github.io/media/icon_huf124860b709ba3b7e308c7ad48690209_1175483_512x512_fill_lanczos_center_3.png</url>
      <title>Rowan Zellers</title>
      <link>https://t-koba-96.github.io/author/rowan-zellers/</link>
    </image>
    
    <item>
      <title>Neural Motifs: Scene Graph Parsing with Global Context</title>
      <link>https://t-koba-96.github.io/publication/neural-motifs/</link>
      <pubDate>Wed, 01 Aug 2018 00:00:00 +0000</pubDate>
      <guid>https://t-koba-96.github.io/publication/neural-motifs/</guid>
      <description>&lt;h1 id=&#34;概要&#34;&gt;&lt;code&gt;概要&lt;/code&gt;&lt;/h1&gt;
&lt;p&gt;シーングラフ生成のタスクで既存の&lt;a href=&#34;../visual-genome&#34;&gt;VisualGenome&lt;/a&gt;データセットにおける実験考察をもとに，新たにLSTMを用いた手法を提案。&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://user-images.githubusercontent.com/38309191/76068490-97a37980-5fd4-11ea-8535-6f52a8d9c9e8.png&#34; alt=&#34;スクリーンショット 2020-03-06 18 02 02&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h2 id=&#34;シーングラフの作成における考察&#34;&gt;シーングラフの作成における考察&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;../visual-genome&#34;&gt;VisualGenome&lt;/a&gt;を用いたシーングラフ生成の過程において，以下の２つの特徴があったと主張。&lt;/p&gt;
&lt;h2 id=&#34;-objectの情報からrelationを更新することはrelationを予測する上で有効であるが逆はそうではない&#34;&gt;⓵ Objectの情報からRelationを更新することはRelationを予測する上で有効であるが，逆はそうではない。&lt;/h2&gt;
&lt;p&gt;下図は，右下に書かれてる四角の中の右半分の情報(Head=始点のObject，Tail=終点のObject，Edge=Relation)を用いて残りの左半分を予測したときのtopK/精度を示したもの。&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://user-images.githubusercontent.com/38309191/76158369-956a2800-6158-11ea-9a39-676a55e028bb.png&#34; alt=&#34;スクリーンショット 2020-03-08 16 19 05&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;見ると，headやtailからedgeを推測する際の精度が高いのに対し，edgeからhead, tailを予測するのは非常に難しくなっているのがわかる。&lt;br&gt;
既存手法ではobjectとrelationの相互関係性に注目し，relationからobject情報を更新していたが，この操作にはあまり意味がないことを主張している。&lt;/p&gt;
&lt;h2 id=&#34;-半分以上の画像において一枚の中に複数回以上登場する同一の関係性motifが存在している&#34;&gt;⓶ 半分以上の画像において，一枚の中に複数回以上登場する同一の関係性（Motif）が存在している&lt;/h2&gt;
&lt;p&gt;動物と手足の関係や，木のように１つではなく大量に存在している物が写っている際，同じ関係性のMotifは何度も登場する可能性が高い。これらについて調査したところ，半分以上の画像においてこのような反復の関係性が確認された。&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://user-images.githubusercontent.com/38309191/76158493-9fd8f180-6159-11ea-9fca-7a353bd0d3c6.png&#34; alt=&#34;スクリーンショット 2020-03-08 16 26 50&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h1 id=&#34;手法&#34;&gt;&lt;code&gt;手法&lt;/code&gt;&lt;/h1&gt;
&lt;p&gt;今までのGraphConvを使った手法(&lt;a href=&#34;../graph-rcnn&#34;&gt;Graph-RCNN&lt;/a&gt;とか)と違いLSTMを用いた手法（下図）。&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://user-images.githubusercontent.com/38309191/76068549-af7afd80-5fd4-11ea-9f76-c4475098d09c.png&#34; alt=&#34;スクリーンショット 2020-03-06 18 02 40&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;構造としては，物体領域検出，物体クラス分類，関係性抽出の3段階構造。(B=BoundingBox, O=CLasslabel,
R=Relation)&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://user-images.githubusercontent.com/38309191/76158755-8dac8280-615c-11ea-8d24-b686ce921610.png&#34; alt=&#34;スクリーンショット 2020-03-08 16 47 48&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Objectcontext予測のあとにEdgecontext予測を持ってきているのは，上記の考察における⓵の理由に基づいている。
以下分けてみていく。&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://user-images.githubusercontent.com/38309191/76158834-7c17aa80-615d-11ea-99a8-95f72ea38336.png&#34; alt=&#34;スクリーンショット 2020-03-08 16 54 28&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h2 id=&#34;-boundingbox&#34;&gt;⓵ BoundingBox&lt;/h2&gt;
&lt;p&gt;BoundingBoxの出力にはFaster-RCNNを利用。Faster-RCNNの出力はboundingboxの他に領域における特徴ベクトルと仮のクラスラベル（次のステップで更新していく）。&lt;/p&gt;
&lt;h2 id=&#34;-objects&#34;&gt;⓶ Objects&lt;/h2&gt;
&lt;p&gt;biLSTMを用いて⓵で得た仮のクラスラベルを更新していく。得たBoundingBoxで得た結果を系列情報として順番にbiLSTMに入力し，最終的なクラスラベルの予測を行う。各層の入力として⓵のクラスラベルの他に，前の入力におけるLSTMの最終出力（クラスのone-hot）を用いる。&lt;/p&gt;
&lt;h2 id=&#34;-relations&#34;&gt;⓷ Relations&lt;/h2&gt;
&lt;p&gt;同様にbiLSTMを用いたedge情報の予測。入力として⓶で得たクラスのone-hotと，⓶のLSTMの隠れ層の出力を用いる。
最後に得た出力同士の全ての組み合わせを考えていき，関係性なしラベルを含んだエッジのラベルを予測させる。⓵で得たboundingbox，⓶で得たクラスラベル，⓷で得たEdgeの関係性ラベルを組み合わせて，最終的にシーングラフが作成できる。&lt;/p&gt;
&lt;h1 id=&#34;実験&#34;&gt;&lt;code&gt;実験&lt;/code&gt;&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;../visual-genome&#34;&gt;VisualGenome&lt;/a&gt;を用いて評価。&lt;/p&gt;
&lt;p&gt;[Scene Graph Detection] &amp;hellip; ラベル情報なしでBBox, Object, Relation 全て予測&lt;br&gt;
[Scene Graph Classification] &amp;hellip; BBoxラベルありでObject, Relation 予測&lt;br&gt;
[Predicate Classification] &amp;hellip; BBox, Objectラベルありで Relation 予測&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://user-images.githubusercontent.com/38309191/76068692-f49f2f80-5fd4-11ea-9839-0bdb1013e0b7.png&#34; alt=&#34;スクリーンショット 2020-03-06 18 04 37&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;生成例&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://user-images.githubusercontent.com/38309191/76068711-fec12e00-5fd4-11ea-8daa-d376668d9803.png&#34; alt=&#34;スクリーンショット 2020-03-06 18 04 54&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h1 id=&#34;新規性&#34;&gt;&lt;code&gt;新規性&lt;/code&gt;&lt;/h1&gt;
&lt;p&gt;LSTMを用いた手法&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>

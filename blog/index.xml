<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>トピック | Bianca Blog</title>
    <link>https://t-koba-96.github.io/blog/</link>
      <atom:link href="https://t-koba-96.github.io/blog/index.xml" rel="self" type="application/rss+xml" />
    <description>トピック</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Wed, 11 Oct 2023 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://t-koba-96.github.io/media/icon_huf124860b709ba3b7e308c7ad48690209_1175483_512x512_fill_lanczos_center_3.png</url>
      <title>トピック</title>
      <link>https://t-koba-96.github.io/blog/</link>
    </image>
    
    <item>
      <title>Graph Convolution について理解</title>
      <link>https://t-koba-96.github.io/blog/graphconv/</link>
      <pubDate>Wed, 11 Oct 2023 00:00:00 +0000</pubDate>
      <guid>https://t-koba-96.github.io/blog/graphconv/</guid>
      <description>&lt;h1 id=&#34;graph&#34;&gt;&lt;code&gt;Graph&lt;/code&gt;&lt;/h1&gt;
&lt;p&gt;Graphとは、ノード(点)とエッジ(線)で構成される構造。&lt;/p&gt;
&lt;img src=&#34;https://github.com/t-koba-96/t-koba-96.github.io/assets/38309191/8cc15c12-0b4a-4c13-8068-ee27a931a485&#34; width=60%&gt;
&lt;p&gt;と表現できる。&lt;/p&gt;
&lt;img src=&#34;https://github.com/t-koba-96/food-101-app/assets/38309191/f1ba6b07-4045-4157-bc52-90d761335e5a&#34; width=60%&gt;
&lt;h2 id=&#34;隣接行列&#34;&gt;&lt;code&gt;隣接行列&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;上記のような適当なグラフ構造について考える。エッジの方向について考えなければ、ノード間のエッジの接続については下位のような隣接行列で表現することができる。尚、対角線上の赤い領域も塗りつぶされているのは、自身への接続(自己ループ)を表している。&lt;/p&gt;
&lt;img src=&#34;https://github.com/t-koba-96/food-101-app/assets/38309191/62457ddb-5e74-403e-8637-7427cd2b9d21&#34; width=60%&gt;
&lt;p&gt;畳み込みの前に、まずは隣接行列を使ってノードの特徴量をグラフ構造に合わせて集約してみる。その場合、単純に隣接行列ノード特徴量の内積を取ればよい(下図。)ただし、集約時に結果を足し合わせてしまうと、ノードの接続数が結果に反映されるため、正規化によって接続数による出力の偏りをなくす必要がある(図では簡略化のため接続数でわって平均を求めている。)&lt;/p&gt;
&lt;img src=&#34;https://github.com/t-koba-96/food-101-app/assets/38309191/a9417e9c-770e-4cbe-b53f-1d493a5410d0&#34; width=60%&gt;
&lt;h2 id=&#34;グラフ畳み込み&#34;&gt;&lt;code&gt;グラフ畳み込み&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;上記では隣接行列(エッジ)の重みを全て均等に扱っていたが、各接続の重みづけを学習することによって、エッジの重みを畳みこみカーネルと見立ててグラフ畳み込みを行う。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>三菱電機との共同研究成果がプレスリリースされました</title>
      <link>https://t-koba-96.github.io/blog/mitsubishi/</link>
      <pubDate>Wed, 16 Oct 2019 00:00:00 +0000</pubDate>
      <guid>https://t-koba-96.github.io/blog/mitsubishi/</guid>
      <description>&lt;h1 id=&#34;概要&#34;&gt;&lt;code&gt;概要&lt;/code&gt;&lt;/h1&gt;
&lt;p&gt;三菱電機との共同研究成果が公開されました。&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://aoki-medialab.jp/information/%e4%b8%89%e8%8f%b1%e9%9b%bb%e6%a9%9f%ef%bc%88%e6%a0%aa%ef%bc%89%e3%81%a8%e3%81%ae%e5%85%b1%e5%90%8c%e7%a0%94%e7%a9%b6%e6%88%90%e6%9e%9c%e3%81%8cceatec2019%e3%81%a7%e5%85%ac%e9%96%8b%e3%81%95%e3%82%8c/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;共同研究成果&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.mitsubishielectric.co.jp/news/2019/pdf/1009.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;三菱電機プレスリリース&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://xtech.nikkei.com/atcl/nxt/news/18/06194/?ST=nnm&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;日経ニューメディア&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>フレーム単位での行動認識結果のデモをFlaskを用いてデプロイ</title>
      <link>https://t-koba-96.github.io/blog/video-demo/</link>
      <pubDate>Wed, 21 Aug 2019 00:00:00 +0000</pubDate>
      <guid>https://t-koba-96.github.io/blog/video-demo/</guid>
      <description>&lt;h1 id=&#34;動画認識デモについて&#34;&gt;&lt;code&gt;動画認識デモについて&lt;/code&gt;&lt;/h1&gt;
&lt;p&gt;今まで作成したデモを汎用的に利用するために，最低限の機能のみに簡略化したデモを作成したので，使い方の手順とともに軽く説明していきたい。
まずはデモの実際の様子を以下に示した。フレーム単位での正解ラベルと予測ラベルを見ることができ，また手動でフレームを選ぶか自動再生するかの2つのモードを切り替えられる。&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://classification-tkoba.appspot.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;link&lt;/a&gt;
&lt;video src=&#34;demo.mp4&#34; controls&gt;&lt;/video&gt;&lt;/p&gt;
&lt;h1 id=&#34;実行までの流れ&#34;&gt;&lt;code&gt;実行までの流れ&lt;/code&gt;&lt;/h1&gt;
&lt;p&gt;次に使い方についても軽く説明していきたい。&lt;/p&gt;
&lt;p&gt;➊&lt;br&gt;
まずはコードを公開している&lt;a href=&#34;https://github.com/t-koba-96/demo-light&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHubページ&lt;/a&gt;に移動し，gitレポジトリのクローンを行う。。&lt;br&gt;
&lt;code&gt;$ git clone [リポジトリ名]&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;➋&lt;br&gt;
gitレポジトリには実際に試すためのサンプルデータ（動画，正解ラベル，認識結果）は載せていないため，別にデータをダウンロードしてもらう必要がある。&lt;a href=&#34;https://github.com/t-koba-96/demo-light/releases&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;URL先&lt;/a&gt;のzipファイル(Assets内のdatas.zip)を解凍し，app.pyと同じ階層にdatasファイルが来るように配置する(元々あるdatasファイルと置き換える)。今回はサンプルデータとして&lt;a href=&#34;http://www.cbi.gatech.edu/fpv/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;gteaデータセット&lt;/a&gt;を用いている。&lt;/p&gt;
&lt;p&gt;➌&lt;br&gt;
必要なコードとデータは揃ったので，最後にアプリを動かすために必要なパッケージのインストールを行う。&lt;br&gt;
&lt;code&gt;$ pip install -r requirements.txt&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;➍&lt;br&gt;
以上で準備は整ったので，アプリを実行する。&lt;br&gt;
&lt;code&gt;$ python app.py&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;➎&lt;br&gt;
&amp;ldquo;Running on [ローカルのURL] &amp;ldquo;，といったメッセージが表示されるので，指定されたURLを開けばアプリが実行されていることが確認できる。&lt;/p&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/video-demo/fact_hu08237754c0b94273cc9e4f128584029d_120360_2fe439c1b2e85c7c2d9c461487a30e17.webp 400w,
               /blog/video-demo/fact_hu08237754c0b94273cc9e4f128584029d_120360_f14347a3311062ae3c716d0054e33cbe.webp 760w,
               /blog/video-demo/fact_hu08237754c0b94273cc9e4f128584029d_120360_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://t-koba-96.github.io/blog/video-demo/fact_hu08237754c0b94273cc9e4f128584029d_120360_2fe439c1b2e85c7c2d9c461487a30e17.webp&#34;
               width=&#34;760&#34;
               height=&#34;54&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;h1 id=&#34;最後に&#34;&gt;&lt;code&gt;最後に&lt;/code&gt;&lt;/h1&gt;
&lt;p&gt;Demo結果の様子だけ見たい場合は&lt;a href=&#34;https://classification-tkoba.appspot.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;リンク先&lt;/a&gt;にあります。
フレームレートがまだまだ遅いので(Google App Engine の仮想マシン環境の影響もある)，より快適な可視化の為に改善をしていきたいとは思っている。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>OpenposeのHandPoseEstimatorを実利用のために改良してみた</title>
      <link>https://t-koba-96.github.io/blog/handpose/</link>
      <pubDate>Wed, 05 Jun 2019 00:00:00 +0000</pubDate>
      <guid>https://t-koba-96.github.io/blog/handpose/</guid>
      <description>&lt;h1 id=&#34;概要&#34;&gt;&lt;code&gt;概要&lt;/code&gt;&lt;/h1&gt;
&lt;p&gt;Project for a simple handpose estimator. Openpose based hand pose estimator searches the hand position based on the skeleton keypoints which starts from the persons head. This means when some keypoints of the upper body is occluded, estimating the handpose will be difficult(Second example). To solve this problem, we combined openpose with TTFNet. By using the TTFNet to estimate the hand position directly, it will be able to estimate handpose in occluded cases too.&lt;/p&gt;
&lt;p&gt;For the code, check &lt;a href=&#34;https://github.com/t-koba-96/handpose-estimator&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub Repository&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>

<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>2 | Bianca Blog</title>
    <link>https://t-koba-96.github.io/publication-type/2/</link>
      <atom:link href="https://t-koba-96.github.io/publication-type/2/index.xml" rel="self" type="application/rss+xml" />
    <description>2</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Sat, 01 Aug 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://t-koba-96.github.io/media/icon_huf124860b709ba3b7e308c7ad48690209_1175483_512x512_fill_lanczos_center_3.png</url>
      <title>2</title>
      <link>https://t-koba-96.github.io/publication-type/2/</link>
    </image>
    
    <item>
      <title>CrossTransformers: spatially-aware few-shot transfer</title>
      <link>https://t-koba-96.github.io/publication/crosstransformers/</link>
      <pubDate>Sat, 01 Aug 2020 00:00:00 +0000</pubDate>
      <guid>https://t-koba-96.github.io/publication/crosstransformers/</guid>
      <description>&lt;h1 id=&#34;概要&#34;&gt;&lt;code&gt;概要&lt;/code&gt;&lt;/h1&gt;
&lt;h2 id=&#34;貢献&#34;&gt;貢献&lt;/h2&gt;
&lt;p&gt;[1] SimCLRを用いた自己表現学習: クラスラベルを用いた学習を行うと、抽出した特徴量は一般化されたクラスの情報のみを保持したものとなり、その過程で局所的な特徴や表現をそぎ落としたものとなってしまう。そこでSimCLR同様の自己表現学習方法を導入する。自身の画像自体を正解レベルとして扱うことで、細部の表現をそぎ落とさないような学習を実現する。&lt;/p&gt;
&lt;p&gt;[2] Transformerによる局所マップ同士の表現の親和性を学習。例えば動物であれば、各体の部位の形や色等、その局所的な特徴の対応性から同一の動物であると決定することができる。Transformerを用いてこのような局所的な特徴同士の比較により、最終的にどのクラスに属しているかの決定を行う。&lt;/p&gt;
&lt;h1 id=&#34;手法&#34;&gt;&lt;code&gt;手法&lt;/code&gt;&lt;/h1&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://user-images.githubusercontent.com/38309191/187329611-db8fa17c-59bf-4059-b3d7-ff6c703a04c3.png&#34; alt=&#34;image&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h1 id=&#34;実験&#34;&gt;&lt;code&gt;実験&lt;/code&gt;&lt;/h1&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://user-images.githubusercontent.com/38309191/187352584-0f526e64-e888-4125-a694-0d4fd26db809.png&#34; alt=&#34;image&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://user-images.githubusercontent.com/38309191/187352620-7cf52b28-00ba-4f1f-88f0-1825dba58e47.png&#34; alt=&#34;image&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h1 id=&#34;新規性&#34;&gt;&lt;code&gt;新規性&lt;/code&gt;&lt;/h1&gt;
</description>
    </item>
    
    <item>
      <title>Attention is all you need</title>
      <link>https://t-koba-96.github.io/publication/attention-is-all-you-need/</link>
      <pubDate>Tue, 01 Aug 2017 00:00:00 +0000</pubDate>
      <guid>https://t-koba-96.github.io/publication/attention-is-all-you-need/</guid>
      <description>&lt;h1 id=&#34;概要&#34;&gt;&lt;code&gt;概要&lt;/code&gt;&lt;/h1&gt;
&lt;p&gt;機械翻訳用のネットワーク，トランスフォーマーの提案。従来LSTMやGRU等のリカレントネットワークや，畳み込みを主に用いていた自然言語の処理だが，アテンションのみを用いて単語間の関連性を考慮するような手法。&lt;/p&gt;
&lt;h1 id=&#34;手法&#34;&gt;&lt;code&gt;手法&lt;/code&gt;&lt;/h1&gt;
&lt;p&gt;ネットワーク全体の概要は以下の通り。&lt;/p&gt;
&lt;img width=&#34;400&#34; alt=&#34;2019-02-22 16 19 00&#34; src=&#34;https://user-images.githubusercontent.com/38309191/75124684-3934ec00-56f4-11ea-924e-ceb279131c5b.png&#34;&gt;
&lt;p&gt;エンコーダ・デコーダ共に，この手法のキモとなるMulti-Head Attentionを残差接続したものを複数層重ねた構造になっている。
図中の Inputsは翻訳前言語の原文, Outputsは学習時においてGroundTruth・推論時はBeginning of sentence，Output Probabilitiesにおいて翻訳した文章を得る。&lt;/p&gt;
&lt;h2 id=&#34;アテンションの構造&#34;&gt;アテンションの構造&lt;/h2&gt;
&lt;p&gt;アテンションの構造は以下のScaled Dot-Product Attention.　元の入力バッチからQ(Query),K(Key),V(Value)の３つを新たに用意して，そのうちQとKを用いて元のValueに掛け合わせるような重みマップ（Attention Map）のようなものを作成してあげる。&lt;/p&gt;
&lt;img width=&#34;400&#34; alt=&#34;2019-02-22 16 20 18&#34; src=&#34;https://user-images.githubusercontent.com/38309191/75125018-2cb19300-56f6-11ea-8b22-64e8126b8aed.png&#34;&gt;
&lt;p&gt;単純にAttentionではなくMulti-Headと表現されているのは，このアテンション計算の際にh分割して計算したのち，最後にまたコンカットしていることを指している。（多分計算コストの削減？精度的な寄与も関係あるかも）&lt;/p&gt;
&lt;img width=&#34;400&#34; alt=&#34;2019-02-22 16 21 40&#34; src=&#34;https://user-images.githubusercontent.com/38309191/75125112-d42ec580-56f6-11ea-8897-627484d38380.png&#34;&gt;
&lt;h2 id=&#34;既存手法に対する優位性&#34;&gt;既存手法に対する優位性&lt;/h2&gt;
&lt;p&gt;畳み込みとかリカレントネットワークに比べて長い時系列情報を見れるよねという話。あとリカレントネットワークは前の情報を引き継いで随時計算するのに対し，アテンションは一気に計算できるので，計算速度も全然早い。&lt;/p&gt;
&lt;img width=&#34;600&#34; alt=&#34;2019-02-22 16 21 40&#34; src=&#34;https://user-images.githubusercontent.com/38309191/75125218-78187100-56f7-11ea-9e6c-5ff03ec2ecd6.png&#34;&gt;
&lt;h1 id=&#34;実験&#34;&gt;&lt;code&gt;実験&lt;/code&gt;&lt;/h1&gt;
&lt;img width=&#34;600&#34; alt=&#34;2019-02-22 16 21 40&#34; src=&#34;https://user-images.githubusercontent.com/38309191/75125333-f6751300-56f7-11ea-8a68-4fd175ff4451.png&#34;&gt;
&lt;h1 id=&#34;新規性&#34;&gt;&lt;code&gt;新規性&lt;/code&gt;&lt;/h1&gt;
&lt;p&gt;アテンションのみによる新たなネットワークの提案。&lt;/p&gt;
&lt;p&gt;State of the art の更新。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Two-Stream Convolutional Networks for Action Recognition in Videos</title>
      <link>https://t-koba-96.github.io/publication/two-stream-convolutional-networks/</link>
      <pubDate>Tue, 01 Jul 2014 00:00:00 +0000</pubDate>
      <guid>https://t-koba-96.github.io/publication/two-stream-convolutional-networks/</guid>
      <description>&lt;h1 id=&#34;概要&#34;&gt;&lt;code&gt;概要&lt;/code&gt;&lt;/h1&gt;
&lt;p&gt;Spatial stream ConvNet(以後Spatial Conv)とTemporal stream ConvNet(以後Temporal Conv)の組み合わせ. Spatial Convでは各フレームの静止画（RGB画像）を入力して畳み込み、空間情報の抽出によるクラス分類. Temporal Convでは各フレームのオプティカルフローを入力して畳み込み、動き情報の抽出によるクラス分類.   下図のように，Spatial Convにおける1つの入力フレームに対して，Temporal ConvではそのフレームからNフレーム分のオプティカルフローを用いる。（RGBとオプティカルフローの入力が1:N）。
最終的な結果は、それぞれのネットワークのクラスの確率分布を統合し、最も高確率のクラスを出力.&lt;/p&gt;
&lt;p&gt;Temporal Convにおいて、オプティカルフローは各フレームにおいてそれぞれ X,Y 方向に2次元配列として入力. よって、入力動画のRGBフレーム数がTの時、入力するオプティカルフローのフレーム数は2NT&lt;/p&gt;
&lt;h1 id=&#34;手法&#34;&gt;&lt;code&gt;手法&lt;/code&gt;&lt;/h1&gt;
&lt;img width=&#34;590&#34; alt=&#34;2018-11-30 18 23 42&#34; src=&#34;https://user-images.githubusercontent.com/38309191/49281727-a4334b00-f4d0-11e8-934d-088a32deb1b5.png&#34;&gt;
&lt;h1 id=&#34;実験&#34;&gt;&lt;code&gt;実験&lt;/code&gt;&lt;/h1&gt;
&lt;p&gt;UCF-101、HMDB-51データセットを用いてハンドクラフト特徴量を用いた行動認識（IDT等）と比較.&lt;/p&gt;
&lt;h2 id=&#34;ucf-101データセット&#34;&gt;UCF-101データセット&lt;/h2&gt;
&lt;img width=&#34;400&#34; alt=&#34;2018-11-30 18 34 21&#34; src=&#34;https://user-images.githubusercontent.com/38309191/49281444-eb6d0c00-f4cf-11e8-9372-f63bb53526b7.png&#34;&gt;
&lt;h2 id=&#34;hmdb-501&#34;&gt;HMDB-501&lt;/h2&gt;
&lt;img width=&#34;400&#34; alt=&#34;2018-11-30 18 46 21&#34; src=&#34;https://user-images.githubusercontent.com/38309191/49281736-abf2ef80-f4d0-11e8-8edb-4dfbeb960410.png&#34;&gt;
&lt;p&gt;またSpatial Conv、Temporal Convそれぞれ単独で用いた場合とも比較.&lt;/p&gt;
&lt;img width=&#34;604&#34; alt=&#34;2018-11-30 18 51 39&#34; src=&#34;https://user-images.githubusercontent.com/38309191/49281864-03915b00-f4d1-11e8-9b05-6e2f15c48694.png&#34;&gt;
&lt;p&gt;UCF-101においては最も高精度. HMDB-51においてはハンドクラフトの方が高精度の場合も&lt;/p&gt;
&lt;h1 id=&#34;新規性&#34;&gt;&lt;code&gt;新規性&lt;/code&gt;&lt;/h1&gt;
&lt;p&gt;・当時はハンドクラフト特徴量による認識が主流の中、深層学習を用いた手法.&lt;/p&gt;
&lt;p&gt;・オプティカルフローを用いることで動画の時系列情報を捉えようとするアプローチ&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>

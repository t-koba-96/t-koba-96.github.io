<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>0 | Bianca Blog</title>
    <link>https://t-koba-96.github.io/publication-type/0/</link>
      <atom:link href="https://t-koba-96.github.io/publication-type/0/index.xml" rel="self" type="application/rss+xml" />
    <description>0</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Mon, 01 Aug 2022 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://t-koba-96.github.io/media/icon_huf124860b709ba3b7e308c7ad48690209_1175483_512x512_fill_lanczos_center_3.png</url>
      <title>0</title>
      <link>https://t-koba-96.github.io/publication-type/0/</link>
    </image>
    
    <item>
      <title>Fast and Unsupervised Action Boundary Detection for Action Segmentation</title>
      <link>https://t-koba-96.github.io/publication/abdetection/</link>
      <pubDate>Mon, 01 Aug 2022 00:00:00 +0000</pubDate>
      <guid>https://t-koba-96.github.io/publication/abdetection/</guid>
      <description>&lt;h1 id=&#34;概要&#34;&gt;&lt;code&gt;概要&lt;/code&gt;&lt;/h1&gt;
&lt;h1 id=&#34;提案手法&#34;&gt;&lt;code&gt;提案手法&lt;/code&gt;&lt;/h1&gt;
&lt;h1 id=&#34;実験&#34;&gt;&lt;code&gt;実験&lt;/code&gt;&lt;/h1&gt;
&lt;h1 id=&#34;新規性&#34;&gt;&lt;code&gt;新規性&lt;/code&gt;&lt;/h1&gt;
</description>
    </item>
    
    <item>
      <title>Temporal-Relational CrossTransformers for Few-Shot Action Recognition</title>
      <link>https://t-koba-96.github.io/publication/tr_crosstransformers/</link>
      <pubDate>Sun, 01 Aug 2021 00:00:00 +0000</pubDate>
      <guid>https://t-koba-96.github.io/publication/tr_crosstransformers/</guid>
      <description>&lt;h1 id=&#34;概要&#34;&gt;&lt;code&gt;概要&lt;/code&gt;&lt;/h1&gt;
&lt;p&gt;CrossTransformerを応用したFew-ShotのActionRecognition手法の提案。&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://user-images.githubusercontent.com/38309191/187157046-d613723d-1f64-4233-96fa-ff09fe31c852.png&#34; alt=&#34;image&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;各学習クラスのサンプルが少ない中で、Queryビデオがどのクラスであるかを認識する。
N-way、K-shotのFSLの時について考える。通常だとQueryビデオがNクラスのうちのどのクラスに属しているか、各クラスのSupportVideo一つずつと比較し、その最大値もしくは平均値を取って判断するが、Transformerのネットワーク構造を用いることですべてのSupportVideoをまとめて見たうえで判断することが可能になるよ、という話。&lt;/p&gt;
&lt;h1 id=&#34;手法&#34;&gt;&lt;code&gt;手法&lt;/code&gt;&lt;/h1&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://user-images.githubusercontent.com/38309191/187158126-ca72e67f-0a31-42c9-b3c3-2e73e202fc68.png&#34; alt=&#34;image&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;各動画は等間隔でフレームサンプリングされたものを使う。QueryVideoからクエリ、SupportVideoからキーとバリューを作成し、&lt;a href=&#34;../22&#34;&gt;CrossTransormer&lt;/a&gt;と同様の構造を用いたAttentionの計算を行う。
この際、上図にはPair representationsとTriple representationsがあるが、Pair representationsでは各動画の中から2フレーム(ペア)のみを用いて特徴量化し、Triple representations では3フレームのみを用いて特徴量化する。そのため、各動画の特徴量をあらわすために扱うのは実際には2~3フレームである(Ablationで4の場合についても検証している）。&lt;/p&gt;
&lt;h1 id=&#34;実験&#34;&gt;&lt;code&gt;実験&lt;/code&gt;&lt;/h1&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://user-images.githubusercontent.com/38309191/187323944-c9d11afd-228d-4789-804e-f4440461ba2f.png&#34; alt=&#34;image&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://user-images.githubusercontent.com/38309191/187324052-fe3ca87b-e2fe-4291-9038-7fd07a7159a8.png&#34; alt=&#34;image&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://user-images.githubusercontent.com/38309191/187324075-6cd5f632-95e4-4343-9064-19260514077b.png&#34; alt=&#34;image&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h1 id=&#34;新規性&#34;&gt;&lt;code&gt;新規性&lt;/code&gt;&lt;/h1&gt;
</description>
    </item>
    
    <item>
      <title>Action Genome: Actions as Composition of Spatio-temporal Scene Graphs</title>
      <link>https://t-koba-96.github.io/publication/actiongenome/</link>
      <pubDate>Sat, 01 Aug 2020 00:00:00 +0000</pubDate>
      <guid>https://t-koba-96.github.io/publication/actiongenome/</guid>
      <description>&lt;h1 id=&#34;概要&#34;&gt;&lt;code&gt;概要&lt;/code&gt;&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;../12&#34;&gt;Visual Genome&lt;/a&gt;の動画版のデータセット。また&lt;a href=&#34;../14&#34;&gt;Feature Banks&lt;/a&gt;を用いた手法の提案。&lt;/p&gt;
&lt;img width=&#34;600&#34; alt=&#34;2019-02-22 16 19 00&#34; src=&#34;featured.png&#34;&gt;  
&lt;h1 id=&#34;手法&#34;&gt;&lt;code&gt;手法&lt;/code&gt;&lt;/h1&gt;
&lt;h2 id=&#34;データセット&#34;&gt;データセット&lt;/h2&gt;
&lt;p&gt;データセットのラベル内容としては動画中のActionの領域とそのCaption，またVisualGenomeのようなSceneGraphが与えられている。&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://user-images.githubusercontent.com/38309191/76069562-67f57100-5fd6-11ea-9f69-bde35dcb6131.png&#34; alt=&#34;スクリーンショット 2020-03-06 18 14 54&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;動画内の各Actionに関して，Actionを均等な時間間隔で分割した際の5フレームを抽出し，その5フレームにおけるシーングラフのラベルを作成。上記の例では１つの動画で４つのアクションが起きているため，4✖️5の合計20フレーム分のシーングラフが存在する。&lt;/p&gt;
&lt;p&gt;他のデータセットとの比較としては以下の通り。&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://user-images.githubusercontent.com/38309191/76069582-7479c980-5fd6-11ea-80ba-692b805558d6.png&#34; alt=&#34;スクリーンショット 2020-03-06 18 15 20&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;動画から各Actionを抽出するAction Recognitionのタスクを考える。同グループの以前の研究である&lt;a href=&#34;../14&#34;&gt;Feature Banks&lt;/a&gt;をベースラインとして用いている。従来のように抽出した3DCNN特徴に加えて，今回のラベルによって得られるシーングラフを予測させてobjectとrelationshipsを捉えたfeatured map を同様にFeature bank として長期の時系列情報として保持することで，行動の分類を行っている。&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://user-images.githubusercontent.com/38309191/76069715-a3903b00-5fd6-11ea-8ec1-ea0443bde2cd.png&#34; alt=&#34;スクリーンショット 2020-03-06 18 16 38&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h1 id=&#34;実験&#34;&gt;&lt;code&gt;実験&lt;/code&gt;&lt;/h1&gt;
&lt;p&gt;提案手法の評価。シーングラフ の利用によって精度の向上を確認。&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://user-images.githubusercontent.com/38309191/76141573-1538b980-60a9-11ea-828d-0f1e8964ecd9.png&#34; alt=&#34;スクリーンショット 2020-03-07 19 23 06&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;既存のシーングラフ生成モデルをActionGenomeに適用した際の精度。既存の手法は画像ベースの手法のため精度も低く，動画用に改善の余地ありとのこと。&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://user-images.githubusercontent.com/38309191/76141570-0c47e800-60a9-11ea-8edb-0c5419ba821a.png&#34; alt=&#34;スクリーンショット 2020-03-07 19 22 53&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h1 id=&#34;新規性&#34;&gt;&lt;code&gt;新規性&lt;/code&gt;&lt;/h1&gt;
&lt;p&gt;動画用のシーングラフを提案。シーングラフを用いたActionRecognitionの手法。&lt;/p&gt;
&lt;h1 id=&#34;コメント&#34;&gt;&lt;code&gt;コメント&lt;/code&gt;&lt;/h1&gt;
&lt;p&gt;公開されたら触ってみたい。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Completeness Modeling and Context Separation for Weakly Supervised Temporal Action Localization</title>
      <link>https://t-koba-96.github.io/publication/cmcs/</link>
      <pubDate>Thu, 01 Aug 2019 00:00:00 +0000</pubDate>
      <guid>https://t-koba-96.github.io/publication/cmcs/</guid>
      <description>&lt;h1 id=&#34;概要&#34;&gt;&lt;code&gt;概要&lt;/code&gt;&lt;/h1&gt;
&lt;p&gt;動画のクラスラベルのみのWeaklyなラベルを用いたTemporal Action Localization.&lt;/p&gt;
&lt;img width=&#34;600&#34; alt=&#34;2019-02-22 16 19 00&#34; src=&#34;featured.png&#34;&gt;  
&lt;h2 id=&#34;weaklyなラベルによる難点&#34;&gt;Weaklyなラベルによる難点&lt;/h2&gt;
&lt;p&gt;Weaklyなラベルによる難点は主に２つあり，解決するための手法を提案。　　&lt;/p&gt;
&lt;h2 id=&#34;-action-completeness-modeling&#34;&gt;⓵ Action completeness modeling&lt;/h2&gt;
&lt;p&gt;下図の上部参照。 例えばPKでは選手がシュートする動きとボールが飛んでいく動きによって構成されるが，Weaklyでは動画のクラス分類を元に解くのでより分類において重要であるシュートのところのみ切り取られやすい。そこでこれらの行動を分けて予測するようなマルチブランチのネットワーク構造にする。&lt;/p&gt;
&lt;h2 id=&#34;-action-context-separation&#34;&gt;⓶ Action-context separation&lt;/h2&gt;
&lt;p&gt;下図の下部参照。行動の前景と背景ではフレーム全体としてのContextはあまり変わらない（どちらにもビリヤード台はずっと写ってる）。これに関しては人物や物の情報がアクションには関わっていると仮定し，OpticalFlowを用いた解決法を提案。&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://user-images.githubusercontent.com/38309191/76378934-1829f780-6392-11ea-8145-b0e018b61c22.png&#34; alt=&#34;スクリーンショット 2020-03-11 12 16 07&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h1 id=&#34;手法&#34;&gt;&lt;code&gt;手法&lt;/code&gt;&lt;/h1&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://user-images.githubusercontent.com/38309191/76379387-43f9ad00-6393-11ea-90fe-582e28b16f52.png&#34; alt=&#34;スクリーンショット 2020-03-11 12 24 30&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;マルチブランチネットワークとアテンションモデルによって構成。マルチブランチNetworkではアクションをK分割して，それぞれの集合（Averageをとる）として求める全体のアクションを得るといった考え方。K分割したブランチにおいてそれぞれで違う部分に注目してもらう必要があるため，DiversityLossを採用。アテンションモデルでは時系列方向にSoftmaxをかけてフレームの重要度を算出。最後にこれらをかけあわせ，動画単位での MILLossをとる。&lt;br&gt;
推論時はマルチブランチをAverageした結果を利用。&lt;/p&gt;
&lt;h1 id=&#34;実験&#34;&gt;&lt;code&gt;実験&lt;/code&gt;&lt;/h1&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://user-images.githubusercontent.com/38309191/76379423-5e338b00-6393-11ea-9281-00de6e0f5ee6.png&#34; alt=&#34;スクリーンショット 2020-03-11 12 25 12&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://user-images.githubusercontent.com/38309191/76379450-6e4b6a80-6393-11ea-9a6e-78045527b853.png&#34; alt=&#34;スクリーンショット 2020-03-11 12 25 40&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h1 id=&#34;新規性&#34;&gt;&lt;code&gt;新規性&lt;/code&gt;&lt;/h1&gt;
&lt;p&gt;マルチブランチによる推定。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Graphical Contrastive Losses for Scene Graph Parsing</title>
      <link>https://t-koba-96.github.io/publication/graphicalcontrastiveloss/</link>
      <pubDate>Thu, 01 Aug 2019 00:00:00 +0000</pubDate>
      <guid>https://t-koba-96.github.io/publication/graphicalcontrastiveloss/</guid>
      <description>&lt;h1 id=&#34;概要&#34;&gt;&lt;code&gt;概要&lt;/code&gt;&lt;/h1&gt;
&lt;p&gt;シーングラフの生成において従来モデルの課題点を指摘した上で，それを改善するための新たなロスを提案し，SoTAを達成。&lt;/p&gt;
&lt;img width=&#34;600&#34; alt=&#34;2019-02-22 16 19 00&#34; src=&#34;featured.png&#34;&gt;  
&lt;h2 id=&#34;シーングラフにおける言葉の定義&#34;&gt;シーングラフにおける言葉の定義&lt;/h2&gt;
&lt;p&gt;まずはじめに論文の内容とは関係ないが，&lt;a href=&#34;../24&#34;&gt;Graph-RCNN&lt;/a&gt;ではあいまいになっていて読みにくかったシーングラフにおける言葉の定義をしっかり記していてくれたのが地味にありがたい。&lt;/p&gt;
&lt;p&gt;・シーングラフを構成するにあたって必要にになるのは，objectとsubject,それをつなぐ関係性であるpredicateと，objectやsubjectに関するattributeの4つ。&lt;br&gt;
&lt;em&gt;&lt;strong&gt;[object, subject, predicate, attribute]&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;・実際に画像上に登場する物体(Object or Subject)は総称してentity.&lt;br&gt;
&lt;em&gt;&lt;strong&gt;[enitity]&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;・最終的に求めるべきobject,subject,predicateの関係性を&amp;lt;s,pred,o&amp;gt;と定義し，これのことをrelationshipsと呼ぶ。&lt;br&gt;
&lt;em&gt;&lt;strong&gt;[relationships]&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;h2 id=&#34;従来手法の課題点&#34;&gt;従来手法の課題点&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;../25&#34;&gt;MotifNet&lt;/a&gt;のように，従来の手法ではまず初めにentityの情報を得てから，それをもとにpredicateをラベルのクラス分類問題として解くことでrelationshipsを求めるという２段階のアプローチが主流である。しかし，このようなアプローチには２つの課題が存在すると主張している。&lt;/p&gt;
&lt;h2 id=&#34;-entity-instance-confusion&#34;&gt;⓵ Entity Instance Confusion&lt;/h2&gt;
&lt;p&gt;同じラベルのentityが複数登場した時，relationshipにおけるenitityがそのうちのどれであるかの判断が難しくなる。以下の例では，男の人が持っているのではない方のワイングラスにobjectが付加されてしまっている。&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://user-images.githubusercontent.com/38309191/76162930-5b634b00-6185-11ea-9618-2ccf2b3978e6.png&#34; alt=&#34;スクリーンショット 2020-03-08 21 39 54&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h2 id=&#34;-proximal-rel-ambiguity&#34;&gt;⓶ Proximal Rel Ambiguity&lt;/h2&gt;
&lt;p&gt;同様に同じラベルのpredicateを持ちうるobjectとsubjectのペアが複数ある時も，どのobjectとsubjectがペアであるかの判別が難しい。以下の例ではplaysというpredicateに対して3人の男と３つの楽器によるペアを判別する必要があるが，正しくマッチングできていないことがみて取れる。&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://user-images.githubusercontent.com/38309191/76162933-6027ff00-6185-11ea-88fc-4cebc7aca47b.png&#34; alt=&#34;スクリーンショット 2020-03-08 21 40 03&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;上記の２つともの問題点が発生する原因として，relationshipsを求める際にそれぞれ独立のクラス分類問題として解いてしまっていることがあげられる。そのため同じ物体や考えられるペアが複数存在してもそれぞれを独立の問題として解いてしまっており，お互いを比較した上でどちらがより正しいかを考慮できていない。&lt;br&gt;
本論文ではそれを解決するための新たなロスを提案している。&lt;/p&gt;
&lt;h2 id=&#34;提案するロス&#34;&gt;提案するロス&lt;/h2&gt;
&lt;p&gt;新たなロス関数では３種類のロスを組み合わせたものとなっている。&lt;/p&gt;
&lt;h2 id=&#34;-class-agnostic-loss&#34;&gt;⓵ CLass Agnostic Loss&lt;/h2&gt;
&lt;p&gt;クラスラベルを考慮しないトリプレットロス。背景（関連なし）クラスを除いた全てのpredicateクラスに関して，以下のような関数として定義する。&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://user-images.githubusercontent.com/38309191/76163467-a4b59980-6189-11ea-9feb-1105c568c435.png&#34; alt=&#34;スクリーンショット 2020-03-08 22 10 34&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;今各entityにおいて，関係性を持つ他のentityとの間のスコアのうち最小のものを最大化し，関係性を持たない他のentityとの間のスコアのうち最大のものを最小化することを考える。そのためには，objectとsubjectに分けるとそれぞれ&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://user-images.githubusercontent.com/38309191/76163471-ac753e00-6189-11ea-9382-7195dcb3e477.png&#34; alt=&#34;スクリーンショット 2020-03-08 22 10 47&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;を最大化することと同義。そのため最小化するロス関数は&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://user-images.githubusercontent.com/38309191/76163474-b39c4c00-6189-11ea-8067-cf13f3e631d0.png&#34; alt=&#34;スクリーンショット 2020-03-08 22 11 00&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;となる。&lt;/p&gt;
&lt;h2 id=&#34;-entity-class-aware-loss&#34;&gt;⓶ Entity Class Aware Loss&lt;/h2&gt;
&lt;p&gt;EntityInstanceConfusionを解決するためのロス。基本的には⓵と同じだが，対応するentityのクラスが同じ中でトリプレットロスを計算する。&lt;/p&gt;
&lt;p&gt;各クラスcに属するenitityに対して&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://user-images.githubusercontent.com/38309191/76163672-88b2f780-618b-11ea-91dd-b7590118d0bd.png&#34; alt=&#34;スクリーンショット 2020-03-08 22 24 05&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;を計算するため，ロス関数は&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://user-images.githubusercontent.com/38309191/76163675-90729c00-618b-11ea-8632-364ef7e3fae9.png&#34; alt=&#34;スクリーンショット 2020-03-08 22 24 19&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;となる。&lt;/p&gt;
&lt;h2 id=&#34;-predicate-class-aware-loss&#34;&gt;⓷ Predicate Class Aware Loss&lt;/h2&gt;
&lt;p&gt;⓶のpredicate版。ProximalRelAmbiguityを解決するためのロス。entityに付加されているpredicateのクラスが同じ中でトリプレットロスを計算する。&lt;/p&gt;
&lt;p&gt;各predicateクラスeが付加されているentityに対して&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://user-images.githubusercontent.com/38309191/76163717-f19a6f80-618b-11ea-9ece-95c1462c1eb3.png&#34; alt=&#34;スクリーンショット 2020-03-08 22 27 01&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;を計算するため，ロス関数は&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://user-images.githubusercontent.com/38309191/76163722-fa8b4100-618b-11ea-9847-4253ee8df19b.png&#34; alt=&#34;スクリーンショット 2020-03-08 22 27 18&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;となる。&lt;/p&gt;
&lt;h2 id=&#34;最終的なロス&#34;&gt;最終的なロス&lt;/h2&gt;
&lt;p&gt;以上３つのロスを(L1,2,3)，従来のcrossentropyloss(L0)に加えることによって学習する。&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://user-images.githubusercontent.com/38309191/76163726-ff4ff500-618b-11ea-986e-ebfba67def58.png&#34; alt=&#34;スクリーンショット 2020-03-08 22 27 26&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h2 id=&#34;モデル&#34;&gt;モデル&lt;/h2&gt;
&lt;p&gt;RelDNと呼ばれるモデルを提案。&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://user-images.githubusercontent.com/38309191/76164082-4b506900-618f-11ea-8fdb-23d7a2c4e348.png&#34; alt=&#34;スクリーンショット 2020-03-08 22 51 01&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Semantic, Spatial, Visualの３つのmoduleを用意し，最後に足し合わせたのちsoftmaxをかけてpredicateを出力。&lt;br&gt;
Visualのうち，relationships全体の特徴量の抽出のみ別に用意したCNN(構造は同じ)を用いて行う。理由としては関係性を捉えるための特徴としてはその限定された領域に注目して欲しく，全体をみてしまうCNNと区別したかったから。（以下図参照）&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://user-images.githubusercontent.com/38309191/76164160-eea17e00-618f-11ea-9cfc-0599176007df.png&#34; alt=&#34;スクリーンショット 2020-03-08 22 55 36&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h1 id=&#34;実験&#34;&gt;&lt;code&gt;実験&lt;/code&gt;&lt;/h1&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://user-images.githubusercontent.com/38309191/76164113-7dfa6180-618f-11ea-884d-be2c85bae456.png&#34; alt=&#34;スクリーンショット 2020-03-08 22 52 27&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://user-images.githubusercontent.com/38309191/76164212-666fa880-6190-11ea-881a-46f2b7445677.png&#34; alt=&#34;スクリーンショット 2020-03-08 22 58 56&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h1 id=&#34;新規性&#34;&gt;&lt;code&gt;新規性&lt;/code&gt;&lt;/h1&gt;
&lt;p&gt;シーングラフ用の新たなロスの提案。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Long-Term Feature Banks for Detailed Video Understanding</title>
      <link>https://t-koba-96.github.io/publication/featurebank/</link>
      <pubDate>Thu, 01 Aug 2019 00:00:00 +0000</pubDate>
      <guid>https://t-koba-96.github.io/publication/featurebank/</guid>
      <description>&lt;h1 id=&#34;概要&#34;&gt;&lt;code&gt;概要&lt;/code&gt;&lt;/h1&gt;
&lt;p&gt;３DCNNでは4秒近くにおける時系列情報しか捉えられない。そこでよりLongTermな情報と組み合わせて考えることでVideoRecognitionの精度が上がりましたよという論文。&lt;/p&gt;
&lt;img width=&#34;655&#34; alt=&#34;スクリーンショット 2020-03-06 12 52 25&#34; src=&#34;https://user-images.githubusercontent.com/38309191/76048917-56e23b00-5fa9-11ea-8618-ca469d4c0bc9.png&#34;&gt;
&lt;h1 id=&#34;手法&#34;&gt;&lt;code&gt;手法&lt;/code&gt;&lt;/h1&gt;
&lt;p&gt;３DCNNの元論文では，何フレームごとかに得た特徴をROI Poolingによってある区間における平均特徴量としたのち，全結合層で分類問題を解く（下図）。しかしこれではShortTermな情報しか捉えられず，ビデオ全体から判断が必要な情報が抜け落ちてしまう。&lt;/p&gt;
&lt;img width=&#34;436&#34; alt=&#34;スクリーンショット 2020-03-06 12 56 58&#34; src=&#34;https://user-images.githubusercontent.com/38309191/76050201-fb647d00-5fa9-11ea-8d3d-ea07d839e0c4.png&#34;&gt;  
&lt;p&gt;そこでFeatureBankOperationを導入し，ShortTermとは別に得たLongTermな情報も同時に捉えるようなネットワークを用意してあげて，そこから得た特徴量も用いて認識を行う。&lt;/p&gt;
&lt;img width=&#34;512&#34; alt=&#34;スクリーンショット 2020-03-06 12 57 15&#34; src=&#34;https://user-images.githubusercontent.com/38309191/76050211-03bcb800-5faa-11ea-8d51-86a5ef139f8d.png&#34;&gt;
&lt;p&gt;FeatureBankOperatorの構造は以下。&lt;/p&gt;
&lt;img width=&#34;636&#34; alt=&#34;スクリーンショット 2020-03-06 12 59 27&#34; src=&#34;https://user-images.githubusercontent.com/38309191/76050307-526a5200-5faa-11ea-8a9a-1ab6a57fb7f7.png&#34;&gt;
&lt;h1 id=&#34;実験&#34;&gt;&lt;code&gt;実験&lt;/code&gt;&lt;/h1&gt;
&lt;img width=&#34;704&#34; alt=&#34;スクリーンショット 2020-03-06 13 02 12&#34; src=&#34;https://user-images.githubusercontent.com/38309191/76050422-b3922580-5faa-11ea-8c98-89f688cd23a4.png&#34;&gt;
&lt;img width=&#34;900&#34; alt=&#34;スクリーンショット 2020-03-06 13 03 27&#34; src=&#34;https://user-images.githubusercontent.com/38309191/76050485-e1776a00-5faa-11ea-8a14-a6204f5e55d4.png&#34;&gt;
&lt;h1 id=&#34;新規性&#34;&gt;&lt;code&gt;新規性&lt;/code&gt;&lt;/h1&gt;
&lt;p&gt;ShortTerm+LongTerm&lt;/p&gt;
&lt;h1 id=&#34;コメント&#34;&gt;&lt;code&gt;コメント&lt;/code&gt;&lt;/h1&gt;
&lt;p&gt;単純な理論だが精度向上に結構寄与してる。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>MS-TCN: Multi-Stage Temporal Convolutional Network for Action Segmentation</title>
      <link>https://t-koba-96.github.io/publication/ms-tcn/</link>
      <pubDate>Thu, 01 Aug 2019 00:00:00 +0000</pubDate>
      <guid>https://t-koba-96.github.io/publication/ms-tcn/</guid>
      <description>&lt;h1 id=&#34;概要&#34;&gt;&lt;code&gt;概要&lt;/code&gt;&lt;/h1&gt;
&lt;p&gt;行動認識のタスクにおいて,&lt;a href=&#34;https://github.com/t-koba-96/paper_summarize/issues/2&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Temporal Convolutional Networks&lt;/a&gt;を複数重ねるMulti-TCNの提案。２個目以降のTCNには各クラスのPredictのSoftmax値を入力とすることで，OverSegmentation（予測結果の頻繁な変化）の抑止を実現。&lt;/p&gt;
&lt;h1 id=&#34;手法&#34;&gt;&lt;code&gt;手法&lt;/code&gt;&lt;/h1&gt;
&lt;p&gt;下図のようにTCNを複数ステージ重ねていき，最終ステージの出力を予測結果として用いる。特徴として，２ステージ目以降の入力は前のステージのSoftmax出力を使う。&lt;br&gt;
イメージとしては，１ステージ目で大まかな行動の認識を行って，２ステージ目以降では時系列情報を考慮しながら細かい行動遷移の予測（主にOversegmentationの抑止）を行っている感じ？&lt;/p&gt;
&lt;img width=&#34;369&#34; alt=&#34;スクリーンショット 2019-04-22 20 25 22&#34; src=&#34;https://user-images.githubusercontent.com/38309191/56498481-d1929b00-653c-11e9-9e08-1e375a0fa2e9.png&#34;&gt;
&lt;h2 id=&#34;ロス&#34;&gt;ロス&lt;/h2&gt;
&lt;p&gt;交差エントロピーに加え，以下のようにフレーム間での正解クラスの予測値の変化を罰している。クラスが本当に切り替わるところは考慮しないよう，閾値を設けている。&lt;/p&gt;
&lt;img width=&#34;294&#34; alt=&#34;スクリーンショット 2019-04-22 20 33 21&#34; src=&#34;https://user-images.githubusercontent.com/38309191/56498721-018e6e00-653e-11e9-89bc-31d3d5fe8e91.png&#34;&gt;
&lt;h1 id=&#34;実験&#34;&gt;&lt;code&gt;実験&lt;/code&gt;&lt;/h1&gt;
&lt;h2 id=&#34;データセット&#34;&gt;データセット&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://cvip.computing.dundee.ac.uk/datasets/foodpreparation/50salads/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;50salads&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;single-tcnとの比較&#34;&gt;Single-TCNとの比較&lt;/h2&gt;
&lt;img width=&#34;353&#34; alt=&#34;スクリーンショット 2019-04-22 20 36 36&#34; src=&#34;https://user-images.githubusercontent.com/38309191/56498806-5cc06080-653e-11e9-8412-199647b033b3.png&#34;&gt;  
&lt;h2 id=&#34;ロスの違いによる比較&#34;&gt;ロスの違いによる比較&lt;/h2&gt;
&lt;img width=&#34;358&#34; alt=&#34;スクリーンショット 2019-04-22 20 36 46&#34; src=&#34;https://user-images.githubusercontent.com/38309191/56498809-5e8a2400-653e-11e9-8cca-71b62d40f6d6.png&#34;&gt;
&lt;h1 id=&#34;新規性&#34;&gt;&lt;code&gt;新規性&lt;/code&gt;&lt;/h1&gt;
&lt;p&gt;・クラス毎のsoftmax値を次のステージへの入力に用いている&lt;/p&gt;
&lt;p&gt;・遷移に制限をかけたロスの提案&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Temporal Cycle-Consistency Learning</title>
      <link>https://t-koba-96.github.io/publication/cycleconsistency/</link>
      <pubDate>Thu, 01 Aug 2019 00:00:00 +0000</pubDate>
      <guid>https://t-koba-96.github.io/publication/cycleconsistency/</guid>
      <description>&lt;h1 id=&#34;概要&#34;&gt;&lt;code&gt;概要&lt;/code&gt;&lt;/h1&gt;
&lt;p&gt;異なる動画においても，同じ動作の場合特徴空間上で近くなるように学習させることで，似た動作の動画同士を同期できるようなマッチングを実現。&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://user-images.githubusercontent.com/38309191/76135346-91131180-6069-11ea-8d1e-bd86a2119762.png&#34; alt=&#34;スクリーンショット 2020-03-07 11 48 27&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h1 id=&#34;手法&#34;&gt;&lt;code&gt;手法&lt;/code&gt;&lt;/h1&gt;
&lt;p&gt;まず動画のペアにおいて，各フレームの埋め込み表現をエンコーダによって得る。このうち１つのフレームに注目したとき（下図の右下赤い点），ペア動画においてその埋め込み表現から最も近いフレームを選ぶ。同様にして元の動画からもうひとつ点を選んだとき，最初の赤い点にもどることが理想であるが，違うフレームが選ばれたときはそこでロスをとることによって正解に近づけていくことを考える。&lt;/p&gt;
&lt;img width=&#34;600&#34; alt=&#34;2019-02-22 16 19 00&#34; src=&#34;featured.png&#34;&gt;  
&lt;p&gt;しかし単純に上のように定義しただけでは損失関数の微分が不可能なため，勾配の計算ができない。そこでペア動画の各フレームにおいて距離空間に応じた重みを計算し，その重み付きの和を計算して，近似した近傍点を求める。同様に，近似した近傍点から元動画の各フレームの距離空間に応じた重みを計算し，その重み付きの和を計算することでロスをとる。&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://user-images.githubusercontent.com/38309191/76135486-af2d4180-606a-11ea-8f5e-70fec33a07df.png&#34; alt=&#34;スクリーンショット 2020-03-07 11 56 26&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h1 id=&#34;実験&#34;&gt;&lt;code&gt;実験&lt;/code&gt;&lt;/h1&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://user-images.githubusercontent.com/38309191/76135690-c40ad480-606c-11ea-8918-00c37b4bb45f.png&#34; alt=&#34;スクリーンショット 2020-03-07 12 11 20&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://user-images.githubusercontent.com/38309191/76135695-cff69680-606c-11ea-99bc-6c65699da600.png&#34; alt=&#34;スクリーンショット 2020-03-07 12 11 38&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h1 id=&#34;新規性&#34;&gt;&lt;code&gt;新規性&lt;/code&gt;&lt;/h1&gt;
&lt;p&gt;動画ペアにおいて，似た動作同士を近づける表現学習におけるCycle-Consistencyなロスの提案。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>VideoBERT: A Joint Model for Video and Language Representation Learning</title>
      <link>https://t-koba-96.github.io/publication/videobert/</link>
      <pubDate>Thu, 01 Aug 2019 00:00:00 +0000</pubDate>
      <guid>https://t-koba-96.github.io/publication/videobert/</guid>
      <description>&lt;h1 id=&#34;概要&#34;&gt;&lt;code&gt;概要&lt;/code&gt;&lt;/h1&gt;
&lt;p&gt;Vision and Language において，多くのモデルでVisionとLanguageでネットワークをそれぞれ用意して同時に学習させていた部分を，&lt;a href=&#34;../06&#34;&gt;Bert&lt;/a&gt;を応用することでマルチモーダルに事前学習させる方法を提案。基本的には&lt;a href=&#34;../06&#34;&gt;Bert&lt;/a&gt;のMask穴埋め問題をLanguage+Visionに拡張したもの。&lt;/p&gt;
&lt;img width=&#34;600&#34; alt=&#34;2019-02-22 16 19 00&#34; src=&#34;featured.png&#34;&gt;  
&lt;p&gt;Downstreamのタスクとしては入力テキスト情報に対応したビデオフレームの出力や，入力ビデオの次におきうるアクションのビデオ出力等が挙げられる。&lt;/p&gt;
&lt;h1 id=&#34;手法&#34;&gt;&lt;code&gt;手法&lt;/code&gt;&lt;/h1&gt;
&lt;p&gt;ネットワーク全体の概要は以下の通り。&lt;/p&gt;
&lt;img width=&#34;800&#34; alt=&#34;スクリーンショット 2020-03-06 12 15 33&#34; src=&#34;https://user-images.githubusercontent.com/38309191/76046767-2fd53a80-5fa4-11ea-818a-03af32cd8904.png&#34;&gt;
&lt;h2 id=&#34;マスク穴埋め問題&#34;&gt;マスク穴埋め問題&lt;/h2&gt;
&lt;p&gt;ランダムで入力をマスクして，出力を予測させることでSelf-supervisedに学習するのはBertと一緒。違いとして，文章同士で入力ではなく，文章と対応するビデオのセットでの入力となる。ビデオ側のトークンはクラスタリングで定義し，４階層✖️12次元の計20736次元の階層的Kmeansによって分類し，言語側と同じくクロスエントロピーをロスとして学習。&lt;/p&gt;
&lt;img width=&#34;618&#34; alt=&#34;スクリーンショット 2020-03-06 12 20 13&#34; src=&#34;https://user-images.githubusercontent.com/38309191/76047018-d6b9d680-5fa4-11ea-9011-73bd036d5fa9.png&#34;&gt;
&lt;h1 id=&#34;実験&#34;&gt;&lt;code&gt;実験&lt;/code&gt;&lt;/h1&gt;
&lt;p&gt;様々なDown Streamタスクでの比較。&lt;/p&gt;
&lt;h2 id=&#34;action-classification&#34;&gt;Action classification&lt;/h2&gt;
&lt;img width=&#34;800&#34; alt=&#34;スクリーンショット 2020-03-06 12 24 30&#34; src=&#34;https://user-images.githubusercontent.com/38309191/76047240-6f505680-5fa5-11ea-906d-27f0afef1bb2.png&#34;&gt;  
&lt;h2 id=&#34;video-captioning&#34;&gt;Video captioning&lt;/h2&gt;
&lt;img width=&#34;800&#34; alt=&#34;スクリーンショット 2020-03-06 12 24 45&#34; src=&#34;https://user-images.githubusercontent.com/38309191/76047253-79725500-5fa5-11ea-9aa3-79f3ee5e5d25.png&#34;&gt;
&lt;img width=&#34;800&#34; alt=&#34;スクリーンショット 2020-03-06 12 24 59&#34; src=&#34;https://user-images.githubusercontent.com/38309191/76047261-80996300-5fa5-11ea-926b-25367f985f12.png&#34;&gt;
&lt;h1 id=&#34;新規性&#34;&gt;&lt;code&gt;新規性&lt;/code&gt;&lt;/h1&gt;
&lt;p&gt;BertのようにSelf-supervisedな事前学習方法の提案によって大きなデータセット規模での学習を可能とし，精度の向上&lt;/p&gt;
&lt;h1 id=&#34;コメント&#34;&gt;&lt;code&gt;コメント&lt;/code&gt;&lt;/h1&gt;
&lt;p&gt;Videoの入力があいまいなイメージ。Bertへの入力としてどのようなルールでフレームを選んでくるかは難しい問題に感じる。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Neural Motifs: Scene Graph Parsing with Global Context</title>
      <link>https://t-koba-96.github.io/publication/neuralmotifs/</link>
      <pubDate>Wed, 01 Aug 2018 00:00:00 +0000</pubDate>
      <guid>https://t-koba-96.github.io/publication/neuralmotifs/</guid>
      <description>&lt;h1 id=&#34;概要&#34;&gt;&lt;code&gt;概要&lt;/code&gt;&lt;/h1&gt;
&lt;p&gt;シーングラフ生成のタスクで既存の&lt;a href=&#34;../12&#34;&gt;VisualGenome&lt;/a&gt;データセットにおける実験考察をもとに，新たにLSTMを用いた手法を提案。&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://user-images.githubusercontent.com/38309191/76068490-97a37980-5fd4-11ea-8535-6f52a8d9c9e8.png&#34; alt=&#34;スクリーンショット 2020-03-06 18 02 02&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h2 id=&#34;シーングラフの作成における考察&#34;&gt;シーングラフの作成における考察&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;../12&#34;&gt;VisualGenome&lt;/a&gt;を用いたシーングラフ生成の過程において，以下の２つの特徴があったと主張。&lt;/p&gt;
&lt;h2 id=&#34;-objectの情報からrelationを更新することはrelationを予測する上で有効であるが逆はそうではない&#34;&gt;⓵ Objectの情報からRelationを更新することはRelationを予測する上で有効であるが，逆はそうではない。&lt;/h2&gt;
&lt;p&gt;下図は，右下に書かれてる四角の中の右半分の情報(Head=始点のObject，Tail=終点のObject，Edge=Relation)を用いて残りの左半分を予測したときのtopK/精度を示したもの。&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://user-images.githubusercontent.com/38309191/76158369-956a2800-6158-11ea-9a39-676a55e028bb.png&#34; alt=&#34;スクリーンショット 2020-03-08 16 19 05&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;見ると，headやtailからedgeを推測する際の精度が高いのに対し，edgeからhead, tailを予測するのは非常に難しくなっているのがわかる。&lt;br&gt;
既存手法ではobjectとrelationの相互関係性に注目し，relationからobject情報を更新していたが，この操作にはあまり意味がないことを主張している。&lt;/p&gt;
&lt;h2 id=&#34;-半分以上の画像において一枚の中に複数回以上登場する同一の関係性motifが存在している&#34;&gt;⓶ 半分以上の画像において，一枚の中に複数回以上登場する同一の関係性（Motif）が存在している&lt;/h2&gt;
&lt;p&gt;動物と手足の関係や，木のように１つではなく大量に存在している物が写っている際，同じ関係性のMotifは何度も登場する可能性が高い。これらについて調査したところ，半分以上の画像においてこのような反復の関係性が確認された。&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://user-images.githubusercontent.com/38309191/76158493-9fd8f180-6159-11ea-9fca-7a353bd0d3c6.png&#34; alt=&#34;スクリーンショット 2020-03-08 16 26 50&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h1 id=&#34;手法&#34;&gt;&lt;code&gt;手法&lt;/code&gt;&lt;/h1&gt;
&lt;p&gt;今までのGraphConvを使った手法(&lt;a href=&#34;../10&#34;&gt;Graph-RCNN&lt;/a&gt;とか)と違いLSTMを用いた手法（下図）。&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://user-images.githubusercontent.com/38309191/76068549-af7afd80-5fd4-11ea-9f76-c4475098d09c.png&#34; alt=&#34;スクリーンショット 2020-03-06 18 02 40&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;構造としては，物体領域検出，物体クラス分類，関係性抽出の3段階構造。(B=BoundingBox, O=CLasslabel,
R=Relation)&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://user-images.githubusercontent.com/38309191/76158755-8dac8280-615c-11ea-8d24-b686ce921610.png&#34; alt=&#34;スクリーンショット 2020-03-08 16 47 48&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Objectcontext予測のあとにEdgecontext予測を持ってきているのは，上記の考察における⓵の理由に基づいている。
以下分けてみていく。&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://user-images.githubusercontent.com/38309191/76158834-7c17aa80-615d-11ea-99a8-95f72ea38336.png&#34; alt=&#34;スクリーンショット 2020-03-08 16 54 28&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h2 id=&#34;-boundingbox&#34;&gt;⓵ BoundingBox&lt;/h2&gt;
&lt;p&gt;BoundingBoxの出力にはFaster-RCNNを利用。Faster-RCNNの出力はboundingboxの他に領域における特徴ベクトルと仮のクラスラベル（次のステップで更新していく）。&lt;/p&gt;
&lt;h2 id=&#34;-objects&#34;&gt;⓶ Objects&lt;/h2&gt;
&lt;p&gt;biLSTMを用いて⓵で得た仮のクラスラベルを更新していく。得たBoundingBoxで得た結果を系列情報として順番にbiLSTMに入力し，最終的なクラスラベルの予測を行う。各層の入力として⓵のクラスラベルの他に，前の入力におけるLSTMの最終出力（クラスのone-hot）を用いる。&lt;/p&gt;
&lt;h2 id=&#34;-relations&#34;&gt;⓷ Relations&lt;/h2&gt;
&lt;p&gt;同様にbiLSTMを用いたedge情報の予測。入力として⓶で得たクラスのone-hotと，⓶のLSTMの隠れ層の出力を用いる。
最後に得た出力同士の全ての組み合わせを考えていき，関係性なしラベルを含んだエッジのラベルを予測させる。⓵で得たboundingbox，⓶で得たクラスラベル，⓷で得たEdgeの関係性ラベルを組み合わせて，最終的にシーングラフが作成できる。&lt;/p&gt;
&lt;h1 id=&#34;実験&#34;&gt;&lt;code&gt;実験&lt;/code&gt;&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;../12&#34;&gt;VisualGenome&lt;/a&gt;を用いて評価。&lt;/p&gt;
&lt;p&gt;[Scene Graph Detection] &amp;hellip; ラベル情報なしでBBox, Object, Relation 全て予測&lt;br&gt;
[Scene Graph Classification] &amp;hellip; BBoxラベルありでObject, Relation 予測&lt;br&gt;
[Predicate Classification] &amp;hellip; BBox, Objectラベルありで Relation 予測&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://user-images.githubusercontent.com/38309191/76068692-f49f2f80-5fd4-11ea-9839-0bdb1013e0b7.png&#34; alt=&#34;スクリーンショット 2020-03-06 18 04 37&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;生成例&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://user-images.githubusercontent.com/38309191/76068711-fec12e00-5fd4-11ea-8daa-d376668d9803.png&#34; alt=&#34;スクリーンショット 2020-03-06 18 04 54&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h1 id=&#34;新規性&#34;&gt;&lt;code&gt;新規性&lt;/code&gt;&lt;/h1&gt;
&lt;p&gt;LSTMを用いた手法&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Weakly-Supervised Action Segmentation with Iterative Soft Boundary Assignment</title>
      <link>https://t-koba-96.github.io/publication/isba/</link>
      <pubDate>Wed, 01 Aug 2018 00:00:00 +0000</pubDate>
      <guid>https://t-koba-96.github.io/publication/isba/</guid>
      <description>&lt;h1 id=&#34;概要&#34;&gt;&lt;code&gt;概要&lt;/code&gt;&lt;/h1&gt;
&lt;p&gt;Action Segmentation のタスクにおいてWeakly-supervisedな手法. 学習用の動画デートセットの正解として，動画内の行動ラベルの順番のみ与える（各フレームにおける正解ラベルはなし）. 同様のWeakly-Supervisedな手法と比較して最高精度を記録。&lt;/p&gt;
&lt;h1 id=&#34;手法&#34;&gt;&lt;code&gt;手法&lt;/code&gt;&lt;/h1&gt;
&lt;p&gt;ネットワーク全体の概要は以下の通り。&lt;/p&gt;
&lt;img width=&#34;400&#34; alt=&#34;2019-02-22 16 19 00&#34; src=&#34;https://user-images.githubusercontent.com/38309191/53226176-9f3ffb00-36bd-11e9-9ce6-ec6e8827dd19.png&#34;&gt;
&lt;p&gt;本手法では，Action Segmentationを行う部分としてTCFPN ，認識結果を元にフレーム毎の正解ラベルの予測を行い，ground truth を更新する部分としてISBAをそれぞれ新たに提案している。&lt;/p&gt;
&lt;h2 id=&#34;tcfpn&#34;&gt;TCFPN&lt;/h2&gt;
&lt;p&gt;Action Segmentationを行う既存手法である&lt;a href=&#34;https://github.com/t-koba-96/papers/issues/2&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ED-TCN&lt;/a&gt; と物体検出のタスクにおいて用いられるFeature Pyramid Networkを組み合わせた手法。単純にEncoder-Decoderのみを使うと，正確な特徴量を抽出できるものの，位置情報（今回の場合時間情報）が大雑把なものとなってしまう。そこで，Encoderの各層を1×1convして加えることで，より正確な位置情報を得ることが可能となる，というイメージ。&lt;/p&gt;
&lt;img width=&#34;400&#34; alt=&#34;2019-02-22 16 20 18&#34; src=&#34;https://user-images.githubusercontent.com/38309191/53226240-d4e4e400-36bd-11e9-9d2a-23bd0c876b9c.png&#34;&gt;
&lt;p&gt;TCFPNでSegmentationを行うには，フレーム毎の行動ラベルが必要なので，動画に対してN個の行動が順に起こるというWeaklyなラベルが与えられた時，動画のフレームをN等分して行動ラベルの初期値として与えてやる。その際0，１のみのOne-hotな表現ではなく，動画の行動が徐々に移り変わるだろうという予測を基に，以下のようなSoft Boundaryなラベル付けを行う。&lt;/p&gt;
&lt;img width=&#34;400&#34; alt=&#34;2019-02-22 16 21 40&#34; src=&#34;https://user-images.githubusercontent.com/38309191/53226296-f80f9380-36bd-11e9-857d-a02678039e80.png&#34;&gt;
&lt;h2 id=&#34;isba&#34;&gt;ISBA&lt;/h2&gt;
&lt;p&gt;TCFPNの出力を元にフレーム毎の正解ラベルの予測，更新を行う部分。要ははじめに与えたN等分するようなフレーム毎の行動の正解ラベルでは正確ではないため，TCFPNの出力を元にフレーム毎の正解を新たに予測し，更新することで実際のground truthに近いラベルを得ようという考え。&lt;/p&gt;
&lt;img width=&#34;450&#34; alt=&#34;2019-02-22 16 22 38&#34; src=&#34;https://user-images.githubusercontent.com/38309191/53226338-18d7e900-36be-11e9-848a-0f4a50c598b2.png&#34;&gt;
&lt;h2 id=&#34;学習テスト&#34;&gt;学習・テスト&lt;/h2&gt;
&lt;p&gt;TCFPNとISBAを繰り返し行い，認識結果を元にフレーム単位の行動ラベルを更新していくことで，フレーム単位の行動ラベルをground truthに近づけることと，Action Segmentationの精度の向上を同時に目指す。 ISBAにおいて独自のロスを導入し，3回連続でロスが小さくならなければ終了し，最もロスの小さかった時の結果を最終出力とする。&lt;/p&gt;
&lt;h1 id=&#34;実験&#34;&gt;&lt;code&gt;実験&lt;/code&gt;&lt;/h1&gt;
&lt;p&gt;Breakfast datasetを用いて他のWeakly-Supervisedな手法との比較.&lt;/p&gt;
&lt;img width=&#34;311&#34; alt=&#34;2019-02-22 16 23 52&#34; src=&#34;https://user-images.githubusercontent.com/38309191/53226421-4886f100-36be-11e9-8ac8-129b6a3637cf.png&#34;&gt;
&lt;h2 id=&#34;学習時の評価&#34;&gt;学習時の評価&lt;/h2&gt;
&lt;img width=&#34;600&#34; alt=&#34;2019-02-22 16 24 43&#34; src=&#34;https://user-images.githubusercontent.com/38309191/53226454-65232900-36be-11e9-9b0c-6fad76b05171.png&#34;&gt;
&lt;h2 id=&#34;テスト時の評価&#34;&gt;テスト時の評価&lt;/h2&gt;
&lt;img width=&#34;600&#34; alt=&#34;2019-02-22 16 25 38&#34; src=&#34;https://user-images.githubusercontent.com/38309191/53226501-86841500-36be-11e9-82f1-1fa6cf13f4ea.png&#34;&gt;
&lt;p&gt;ちなみにfully-Supervisedな時の提案手法のaccuracyは52.0&lt;/p&gt;
&lt;h1 id=&#34;新規性&#34;&gt;&lt;code&gt;新規性&lt;/code&gt;&lt;/h1&gt;
&lt;p&gt;・新たなWeakly-Supervisedな手法の提案。それにより最高精度を記録。&lt;/p&gt;
&lt;p&gt;・行動認識の結果を元に正解ラベルを更新していくという発想。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Temporal Convolutional Networks for Action Segmentation and Detection</title>
      <link>https://t-koba-96.github.io/publication/tcn/</link>
      <pubDate>Tue, 01 Aug 2017 00:00:00 +0000</pubDate>
      <guid>https://t-koba-96.github.io/publication/tcn/</guid>
      <description>&lt;h1 id=&#34;概要&#34;&gt;&lt;code&gt;概要&lt;/code&gt;&lt;/h1&gt;
&lt;p&gt;詳細な行動認識のタスクにおいて、動画の各フレームから抽出した特徴を用いて、長期的な時系列情報を考慮するように畳み込みを行って認識を行うネットワークであるTemporal Convolutional Network(以後TCN)の提案.&lt;br&gt;
２種類のTCN , Encoder-Decoder TCN(以後ED-TCN) とDilated TCNを提案している.&lt;/p&gt;
&lt;h1 id=&#34;手法&#34;&gt;&lt;code&gt;手法&lt;/code&gt;&lt;/h1&gt;
&lt;p&gt;提案した両方のTCNにおいて、入力は各フレームから何かしらのCNN(Resnet,VGG等)を用いて抽出した特徴量を時系列順に並べたものであり、出力としては行動クラス結果が各フレームごとに出力される.&lt;/p&gt;
&lt;h2 id=&#34;ed-tcn&#34;&gt;ED-TCN&lt;/h2&gt;
&lt;p&gt;エンコーダとデコーダを組み合わせたTCN. エンコーダで畳み込み＋MaxPoolingを行うことで時系列を考慮した特徴抽出、その後デコーダで畳み込み＋アップサンプリングを行うことで各フレームごとの行動クラス確率分布を得る.&lt;/p&gt;
&lt;img width=&#34;400&#34; alt=&#34;2018-11-30 22 19 28&#34; src=&#34;https://user-images.githubusercontent.com/38309191/49291542-0babc380-f4ee-11e8-9912-026b18f4d8bb.png&#34;&gt;
&lt;h2 id=&#34;dilated-tcn&#34;&gt;Dilated-TCN&lt;/h2&gt;
&lt;p&gt;時系列の畳み込みとしてDilated Convolutionを用いた手法. Dilated Convolutionとは簡単に説明すると、入力の間隔を空けて(間を0とみなす)畳み込みを行うこと(d=1なら間隔なし、d=２なら間隔１、d=4なら間隔３). dの値をどんどん(本論文では指数関数的に)大きくしていったDilated Convolution層を重ねることで、より長期的な時系列の範囲で特徴抽出を行うことが可能となる. またスキップコネクションの導入で層を深くすることを可能に。&lt;/p&gt;
&lt;img width=&#34;400&#34; alt=&#34;2018-11-30 22 20 33&#34; src=&#34;https://user-images.githubusercontent.com/38309191/49291590-2bdb8280-f4ee-11e8-9444-01090dd85fe5.png&#34;&gt;
&lt;h1 id=&#34;実験&#34;&gt;&lt;code&gt;実験&lt;/code&gt;&lt;/h1&gt;
&lt;p&gt;50salads、MERL Shopping、GTEAの３種のデータセットを用いて、Spatial CNN等との比較&lt;/p&gt;
&lt;img width=&#34;600&#34; alt=&#34;2018-11-30 21 58 12&#34; src=&#34;https://user-images.githubusercontent.com/38309191/49291839-e53a5800-f4ee-11e8-9325-46725b25cd76.png&#34;&gt;
&lt;img width=&#34;374&#34; alt=&#34;2018-11-30 22 27 34&#34; src=&#34;https://user-images.githubusercontent.com/38309191/49291938-292d5d00-f4ef-11e8-9b6f-afef4c731408.png&#34;&gt;
&lt;h1 id=&#34;新規性&#34;&gt;&lt;code&gt;新規性&lt;/code&gt;&lt;/h1&gt;
&lt;p&gt;・各層ごとに同時に処理を行うため、入力フレーム数が増えても処理時間に影響をあまり与えない.&lt;br&gt;
　→フレームごとに処理を行うLSTM等と比較して高速な処理&lt;/p&gt;
&lt;p&gt;・３Dconvは動画全体(数フレーム分)に対して1つの行動クラスの出力であるのに対し、TCNでは各フレームごとに行動クラスの出力が可能&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
